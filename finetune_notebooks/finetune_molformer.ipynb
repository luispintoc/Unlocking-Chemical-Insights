{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install PyTDC\n",
    "# ! pip install transformers==4.46.3\n",
    "\n",
    "# ! pip uninstall -y torchvision\n",
    "# ! pip cache purge\n",
    "# ! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import MSELoss, BCEWithLogitsLoss\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tdc.benchmark_group import admet_group\n",
    "from tdc.single_pred import ADME, Tox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_canonical(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return Chem.MolToSmiles(mol, isomericSmiles=True)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, smiles_col, target_col=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.smiles_col = smiles_col\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        smiles = row[self.smiles_col]\n",
    "        if self.target_col is not None:\n",
    "            target = row[self.target_col]\n",
    "        return smiles, target # if self.target_col is not None else None\n",
    "\n",
    "def create_collator(tokenizer):\n",
    "    def collator(batch):\n",
    "        try:\n",
    "            smiles, targets = zip(*batch)\n",
    "            encodings = tokenizer(smiles, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "            input_ids = encodings['input_ids']\n",
    "            attention_mask = encodings['attention_mask']\n",
    "            targets = np.array(targets)\n",
    "            return input_ids, attention_mask, torch.tensor(targets) # if targets is not None else None\n",
    "        except Exception as e:\n",
    "            print('Error in collator:')\n",
    "            print(e)\n",
    "            return None\n",
    "    return collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True)\n",
    "collator = create_collator(tokenizer)\n",
    "base_dir = '../input_data/tdcommons/admet_group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                reg_drop_rate=0.1,\n",
    "                intermediate_size=256,\n",
    "                num_targets=1):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        self.reg_drop_rate = reg_drop_rate\n",
    "        self.num_targets = num_targets\n",
    "        self.intermediate_size = intermediate_size\n",
    "\n",
    "        self.hidden_size = 768\n",
    "        self.transformer = AutoModel.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", deterministic_eval=True, trust_remote_code=True)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(self.reg_drop_rate),\n",
    "            nn.Linear(self.hidden_size, self.intermediate_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.reg_drop_rate),\n",
    "            nn.Linear(self.intermediate_size, self.num_targets)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, layer_idx=-1):\n",
    "        model_out = self.transformer(input_ids, attention_mask, output_hidden_states=True)\n",
    "        embeddings = (\n",
    "            attention_mask.unsqueeze(2) * model_out.hidden_states[layer_idx]\n",
    "        ).sum(dim=1) / attention_mask.sum(dim=1).unsqueeze(1)\n",
    "        output = self.regressor(embeddings)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tasks = ['pgp_broccatelli', 'bioavailability_ma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for task in tasks:\n",
    "\n",
    "    if task.startswith('.'):\n",
    "        continue\n",
    "\n",
    "    print(task)\n",
    "\n",
    "    prefix = 'tdcommons/'\n",
    "    if prefix+task in utils.tdc_mae_tasks:\n",
    "        metric = 'mae'\n",
    "    elif prefix+task in utils.tdc_spearman_task:\n",
    "        metric = 'spearman'\n",
    "    elif prefix+task in utils.polaris_pearson_tasks:\n",
    "        metric = 'pearson'\n",
    "    elif prefix+task in utils.tdc_auroc_tasks:\n",
    "        metric = 'auc'\n",
    "    elif prefix+task in utils.tdc_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in utils.tdc_aucpr2_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in utils.polaris_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    else:\n",
    "        raise ValueError(f\"Task {task} not found in any known task list.\")\n",
    "\n",
    "    try:\n",
    "        data = ADME(name = task)\n",
    "    except:\n",
    "        data = Tox(name = task)\n",
    "\n",
    "    split = data.get_split(method = 'scaffold')\n",
    "    \n",
    "    train_df = split['train'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    val_df = split['valid'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    test_df = split['test'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    \n",
    "    train_df['smiles'] = train_df['smiles'].apply(to_canonical)\n",
    "    val_df['smiles'] = val_df['smiles'].apply(to_canonical)\n",
    "    test_df['smiles'] = test_df['smiles'].apply(to_canonical)\n",
    "\n",
    "    if metric in ('mae', 'spearman', 'pearson'):\n",
    "        scaler = StandardScaler()\n",
    "        # fit only on train targets\n",
    "        train_vals = train_df[['target']].values\n",
    "        scaler.fit(train_vals)\n",
    "        # add scaled targets\n",
    "        train_df['target_scaled'] = scaler.transform(train_vals)\n",
    "        val_df['target_scaled']   = scaler.transform(val_df[['target']].values)\n",
    "        # keep original test targets aside for final metrics\n",
    "        test_targets_orig = test_df['target'].values\n",
    "        test_df['target_scaled']  = scaler.transform(test_df[['target']].values)\n",
    "        target_col = 'target_scaled'\n",
    "    else:\n",
    "        target_col = 'target'\n",
    "    \n",
    "    \n",
    "    transformer = AutoModel.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", deterministic_eval=True, trust_remote_code=True)\n",
    "    \n",
    "    train_dataset = CustomDataset(train_df, tokenizer, 'smiles', target_col)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, collate_fn=collator, shuffle=True, drop_last=True)\n",
    "\n",
    "    val_dataset = CustomDataset(val_df, tokenizer, 'smiles',target_col)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=512, collate_fn=collator, shuffle=False, drop_last=False)\n",
    "    \n",
    "    test_dataset = CustomDataset(test_df, tokenizer, 'smiles', target_col)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=512, collate_fn=collator, shuffle=False, drop_last=False)\n",
    "    \n",
    "    num_layers = transformer.config.num_hidden_layers + 1\n",
    "    layer_indices = list(range(1, num_layers))\n",
    "\n",
    "    test_metrics = []\n",
    "    for layer_idx in layer_indices:\n",
    "        print(f'Layer: {layer_idx}')\n",
    "\n",
    "        # hyperparameter sweep over learning rates\n",
    "        for lr in [1e-5, 2e-5, 5e-5, 1e-4, 2e-4]:\n",
    "            print(f'LR={lr}')\n",
    "            model = Transformer().to('cuda')\n",
    "            optimizer = AdamW(model.parameters(), lr=lr)\n",
    "            epochs = 50\n",
    "            num_training_steps = len(train_dataloader) * epochs\n",
    "            num_warmup_steps = int(0.05 * num_training_steps)\n",
    "            scheduler_warmup = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "            # choose loss fn by task type\n",
    "            if metric in ('mae', 'spearman', 'pearson'):\n",
    "                loss_fn = MSELoss()\n",
    "            else:\n",
    "                loss_fn = BCEWithLogitsLoss()\n",
    "            # For MAE we want to minimize; for others maximize\n",
    "            if metric == 'mae':\n",
    "                best_val = float('inf')\n",
    "            else:\n",
    "                best_val = -float('inf')\n",
    "            best_state = None\n",
    "            best_epoch = -1\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                for input_ids, attention_mask, targets in train_dataloader:\n",
    "                    input_ids = input_ids.to('cuda')\n",
    "                    attention_mask = attention_mask.to('cuda')\n",
    "                    targets = targets.to('cuda').float()\n",
    "                    optimizer.zero_grad()\n",
    "                    preds = model(input_ids, attention_mask, layer_idx).squeeze()\n",
    "                    loss = loss_fn(preds, targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler_warmup.step()\n",
    "\n",
    "                # evaluate on validation set\n",
    "                model.eval()\n",
    "                val_preds, val_targs = [], []\n",
    "                with torch.no_grad():\n",
    "                    for input_ids, attention_mask, targets in val_dataloader:\n",
    "                        input_ids = input_ids.to('cuda')\n",
    "                        attention_mask = attention_mask.to('cuda')\n",
    "                        preds = model(input_ids, attention_mask, layer_idx) \\\n",
    "                                    .squeeze(-1) \\\n",
    "                                    .cpu() \\\n",
    "                                    .numpy()\n",
    "                        val_preds.extend(preds.tolist())\n",
    "                        val_targs.extend(targets.numpy().tolist())\n",
    "\n",
    "                # compute your chosen metric\n",
    "                if metric == 'mae':\n",
    "                    val_score = np.mean(np.abs(np.array(val_preds) - np.array(val_targs)))\n",
    "                elif metric == 'spearman':\n",
    "                    val_score = spearmanr(val_targs, val_preds)[0]\n",
    "                elif metric == 'pearson':\n",
    "                    val_score = pearsonr(val_targs, val_preds)[0]\n",
    "                elif metric == 'auc':\n",
    "                    val_score = roc_auc_score(val_targs, val_preds)\n",
    "                elif metric == 'aucpr':\n",
    "                    val_score = average_precision_score(val_targs, val_preds)\n",
    "\n",
    "                improved = (metric == 'mae' and val_score < best_val) or (metric != 'mae' and val_score > best_val)\n",
    "                if improved:\n",
    "                    best_val   = val_score\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    best_epoch = epoch\n",
    "                    # print(f\"    â†³ new best val_{metric}: {best_val:.4f} (epoch {best_epoch})\")\n",
    "\n",
    "        # now evaluate that best model on the test set\n",
    "        model = Transformer().to('cuda')\n",
    "        model.load_state_dict(best_state)\n",
    "        model.eval()\n",
    "        test_preds, test_targs = [], []\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, targets in test_dataloader:\n",
    "                input_ids = input_ids.to('cuda')\n",
    "                attention_mask = attention_mask.to('cuda')\n",
    "                preds = model(input_ids, attention_mask, layer_idx) \\\n",
    "                            .squeeze(-1) \\\n",
    "                            .cpu() \\\n",
    "                            .numpy()\n",
    "                test_preds.extend(preds.tolist())\n",
    "                test_targs.extend(targets.numpy().tolist())\n",
    "\n",
    "        if metric in ('mae', 'spearman', 'pearson'):\n",
    "            # bring preds back to original units\n",
    "            test_preds = scaler.inverse_transform(\n",
    "                np.array(test_preds).reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            # use original test targets (unscaled)\n",
    "            test_targs = test_targets_orig\n",
    "\n",
    "        if metric == 'mae':\n",
    "            test_score = np.mean(np.abs(np.array(test_preds) - np.array(test_targs)))\n",
    "        elif metric == 'spearman':\n",
    "            test_score = spearmanr(test_targs, test_preds)[0]\n",
    "        elif metric == 'pearson':\n",
    "            test_score = pearsonr(test_targs, test_preds)[0]\n",
    "        elif metric == 'auc':\n",
    "            test_score = roc_auc_score(test_targs, test_preds)\n",
    "        elif metric == 'aucpr':\n",
    "            test_score = average_precision_score(test_targs, test_preds)\n",
    "\n",
    "        print(f\"Layer {layer_idx}: {test_score}\")\n",
    "        test_metrics.append(test_score)\n",
    "\n",
    "    # save a DataFrame with one column per task and rows = layer indices\n",
    "    results_df = pd.DataFrame({task: test_metrics}, index=layer_indices)\n",
    "    dfs.append(results_df)\n",
    "    results_df.to_csv(f\"tmp/molf_{task}_layer_results.csv\", index=False)\n",
    "\n",
    "# dfs = pd.concat(dfs, axis=1)\n",
    "# dfs.to_csv('./results_molf_finetune.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
