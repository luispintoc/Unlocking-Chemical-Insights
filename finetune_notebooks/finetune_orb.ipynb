{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0805dab-b886-476a-8741-b662f94da1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "import os, sys, gc\n",
    "import pandas as pd\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import ase\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import MSELoss, BCEWithLogitsLoss\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "import orb_models.utils as utils\n",
    "from orb_models.forcefield.base import AtomGraphs\n",
    "from orb_models.dataset.augmentations import rotate_randomly\n",
    "from orb_models.dataset.base_datasets import AtomsDataset\n",
    "\n",
    "sys.path.append('../external_repos/')\n",
    "from orb_models_modified.orb_models.forcefield import base, pretrained, atomic_system, property_definitions\n",
    "\n",
    "sys.path.append('../')\n",
    "import utils as tdc_utils\n",
    "\n",
    "from tdc.single_pred import ADME, Tox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18669cdf-48c3-4ec7-bde0-8fb20c07a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tdc.benchmark_group import admet_group\n",
    "\n",
    "# group = admet_group(path = 'tdcommons/')\n",
    "# benchmark = group.get('ames')\n",
    "\n",
    "# name = benchmark['name']\n",
    "# train_val, test = benchmark['train_val'], benchmark['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde844c-f17c-425e-ac3e-81fab86e04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def smiles_to_xyz_file(smiles: str, output_dir: str = 'xyz_files/', random_seed: int = 42,\n",
    "                      jitter_amp: float = 1e-2) -> str | None:\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert SMILES to 3D structure with hydrogens (for better geometry)\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "\n",
    "    mol = Chem.AddHs(mol)\n",
    "\n",
    "    # Try different embedding methods in order of preference\n",
    "    conformer_generated = False\n",
    "    \n",
    "    # Method 1: Standard embedding with UFF optimization\n",
    "    try:\n",
    "        embed_result = AllChem.EmbedMolecule(mol, maxAttempts=2000, randomSeed=random_seed, useRandomCoords=True)\n",
    "        if embed_result == 0:  # Success\n",
    "            # UFF optimization\n",
    "            uff_props = AllChem.UFFGetMoleculeForceField(mol)\n",
    "            if uff_props is not None:\n",
    "                uff_props.Initialize()\n",
    "                uff_props.Minimize(maxIts=1000)\n",
    "                \n",
    "                # Add jitter to avoid symmetry issues\n",
    "                conf = mol.GetConformer()\n",
    "                pos = conf.GetPositions()\n",
    "                np.random.seed(random_seed)\n",
    "                noise = (np.random.rand(*pos.shape) - 0.5) * jitter_amp\n",
    "                new_pos = pos + noise\n",
    "                for i, p in enumerate(new_pos):\n",
    "                    conf.SetAtomPosition(i, p.tolist())\n",
    "                \n",
    "                # Final MMFF optimization\n",
    "                AllChem.MMFFOptimizeMolecule(mol)\n",
    "                conformer_generated = True\n",
    "    except Exception as e:\n",
    "        print(f\"Method 1 failed: {e}\")\n",
    "        pass\n",
    "    \n",
    "    # Method 2: Try ETKDGv3 if method 1 failed\n",
    "    if not conformer_generated:\n",
    "        try:\n",
    "            embed_result = AllChem.EmbedMolecule(mol, AllChem.ETKDGv3())\n",
    "            if embed_result == 0:\n",
    "                conformer_generated = True\n",
    "        except Exception as e:\n",
    "            print(f\"Method 2 failed: {e}\")\n",
    "            pass\n",
    "    \n",
    "    # Method 3: Try explicit conformer creation with 2D coords\n",
    "    if not conformer_generated:\n",
    "        try:\n",
    "            # Create a fresh conformer explicitly\n",
    "            conf = Chem.Conformer(mol.GetNumAtoms())\n",
    "            for i in range(mol.GetNumAtoms()):\n",
    "                conf.SetAtomPosition(i, [0.0, 0.0, 0.0])  # Initialize with zeros\n",
    "            conf_id = mol.AddConformer(conf)\n",
    "            \n",
    "            # Now compute 2D coords\n",
    "            AllChem.Compute2DCoords(mol)\n",
    "            \n",
    "            # Check if the conformer exists and has non-zero coordinates\n",
    "            if mol.GetNumConformers() > 0:\n",
    "                conformer_generated = True\n",
    "        except Exception as e:\n",
    "            print(f\"Method 3 failed: {e}\")\n",
    "            pass\n",
    "\n",
    "    # Bail out if we couldn't generate any conformer\n",
    "    if not conformer_generated or mol.GetNumConformers() == 0:\n",
    "        return None\n",
    "\n",
    "    # Get the conformer\n",
    "    conf = mol.GetConformer()\n",
    "\n",
    "    # 5) Check for overlaps & retry if needed\n",
    "    def has_overlaps(pos, eps=1e-6):\n",
    "        d = torch.cdist(torch.tensor(pos), torch.tensor(pos))\n",
    "        n = d.shape[0]\n",
    "        d[range(n), range(n)] = float('inf')\n",
    "        return (d <= eps).any().item()\n",
    "\n",
    "    pos = conf.GetPositions()\n",
    "    if has_overlaps(pos, eps=1e-6):\n",
    "        # you could loop a few times here with new seeds,\n",
    "        # or simply bail out and let your dataset drop it:\n",
    "        return None\n",
    "    \n",
    "    # Extract heavy atoms only\n",
    "    heavy_atoms = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        if atom.GetSymbol() != 'H':\n",
    "            idx = atom.GetIdx()\n",
    "            try:\n",
    "                pos = conf.GetAtomPosition(idx)\n",
    "                heavy_atoms.append((atom.GetSymbol(), pos))\n",
    "            except Exception:\n",
    "                # Skip this atom if position can't be retrieved\n",
    "                continue\n",
    "\n",
    "    # If no heavy atoms were successfully processed, fail\n",
    "    if not heavy_atoms:\n",
    "        return None\n",
    "\n",
    "    # Build filename\n",
    "    h = hashlib.md5(smiles.encode()).hexdigest()[:10]\n",
    "    filename = f\"{h}.xyz\"\n",
    "    path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Write out XYZ\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(f\"{len(heavy_atoms)}\\n\")\n",
    "        f.write(f\"{smiles}\\n\")\n",
    "        for sym, pt in heavy_atoms:\n",
    "            f.write(f\"{sym} {pt.x:.6f} {pt.y:.6f} {pt.z:.6f}\\n\")\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d81605-43ff-41b7-b7ef-f5e4e52f5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_smiles(smiles):\n",
    "    \"\"\"Converts a SMILES string to its canonical form.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None  # or raise an exception, depending on your needs\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8072b3-a0b7-4588-b820-524c5222e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smiles(input_file):\n",
    "    # Read the entire content of the XYZ file.\n",
    "    with open(input_file, 'r') as f:\n",
    "        xyz_data = f.read()\n",
    "\n",
    "    # Split the content into lines.\n",
    "    lines = xyz_data.splitlines()\n",
    "    smiles = lines[1].strip()\n",
    "\n",
    "    return smiles #canonicalize_smiles(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6526a789-0ad6-4dc2-a82f-7a4c7afbbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XYZFolderDataset(AtomsDataset):\n",
    "    \"\"\"\n",
    "    A Dataset that mimics AseSqliteDataset but reads single .xyz files\n",
    "    based on metal_smiles from a DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,                        # your pandas DataFrame\n",
    "        xyz_dir: Union[str, Path],\n",
    "        system_config: atomic_system.SystemConfig,\n",
    "        target_col: str,\n",
    "        target_config: Optional[property_definitions.PropertyConfig] = None,\n",
    "        augmentations: Optional[List[Callable[[ase.Atoms], None]]] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ):\n",
    "        super().__init__(name=\"xyz_folder\", system_config=system_config, augmentations=augmentations)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.xyz_dir = Path(xyz_dir)\n",
    "        self.target_col = target_col\n",
    "        self.dtype = dtype or torch.get_default_dtype()\n",
    "        self.target_config = target_config or property_definitions.PropertyConfig()\n",
    "        self.constraints: List[Callable] = []\n",
    "\n",
    "        # build smiles → path lookup\n",
    "        self.xyz_dir = Path(xyz_dir)\n",
    "        lookup = {}\n",
    "        for p in self.xyz_dir.glob(\"*.xyz\"):\n",
    "            s = get_smiles(str(p))\n",
    "            lookup[s] = p\n",
    "        self._lookup = lookup\n",
    "\n",
    "        df = df.reset_index(drop=True)\n",
    "        mask = df['smiles'].isin(lookup)\n",
    "        num_bad = (~mask).sum()\n",
    "        if num_bad > 0:\n",
    "            print(f\"Warning: dropping {num_bad} rows with no matching .xyz file\")\n",
    "        self.df = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[AtomGraphs, torch.Tensor]:\n",
    "        row = self.df.iloc[idx]\n",
    "        smiles = row['smiles']\n",
    "        target = row[self.target_col]\n",
    "\n",
    "        xyz_path = self._lookup.get(smiles)\n",
    "        if xyz_path is None:\n",
    "            raise KeyError(f\"Could not find .xyz for SMILES `{smiles}` in {self.xyz_dir}\")\n",
    "\n",
    "        # read Atoms\n",
    "        atoms = ase.io.read(str(xyz_path))\n",
    "        # extract targets into atoms.info\n",
    "        atoms.info = {}\n",
    "        atoms.info.update(self.target_config.extract(row.to_dict(), self.name, \"targets\"))\n",
    "\n",
    "        # augmentations\n",
    "        for aug in self.augmentations or []:\n",
    "            aug(atoms)\n",
    "\n",
    "        # constraints\n",
    "        atoms.set_constraint(None)\n",
    "        for c in self.constraints:\n",
    "            c(atoms, {}, self.name)\n",
    "\n",
    "        # to graph\n",
    "        graph = atomic_system.ase_atoms_to_atom_graphs(\n",
    "            atoms=atoms,\n",
    "            system_config=self.system_config,\n",
    "            edge_method=\"knn_scipy\",\n",
    "            wrap=True,\n",
    "            system_id=idx,\n",
    "            output_dtype=self.dtype,\n",
    "            graph_construction_dtype=self.dtype,\n",
    "        )\n",
    "\n",
    "        # return graph + scalar target\n",
    "        return graph, torch.tensor(target, dtype=self.dtype), idx\n",
    "\n",
    "    def get_atom(self, idx: int) -> ase.Atoms:\n",
    "        \"\"\"Return the raw ASE Atoms for this index.\"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "        xyz_path = self._lookup[row['smiles']]\n",
    "        return ase.io.read(str(xyz_path))\n",
    "\n",
    "    def get_atom_and_metadata(self, idx: int) -> Tuple[ase.Atoms, Dict]:\n",
    "        \"\"\"Return both ASE Atoms and the row’s metadata dict.\"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "        xyz_path = self._lookup[row['smiles']]\n",
    "        atoms = ase.io.read(str(xyz_path))\n",
    "        return atoms, row.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ebe037-ff92-43e0-8daa-d05798fbff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_graph_and_targets(batch):\n",
    "    # batch is a list of (AtomGraphs, tensor) tuples\n",
    "    graphs, targets, idxs = zip(*batch)\n",
    "    batched_graph = base.batch_graphs(graphs)\n",
    "    batched_targets = torch.stack(targets, dim=0)\n",
    "    return batched_graph, batched_targets, torch.tensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe741e6-f968-405f-a6ba-2dab7c265088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_loader(\n",
    "    df,\n",
    "    xyz_dir: str,\n",
    "    system_config,\n",
    "    target_col: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    target_config=None,\n",
    "    augmentations=None,\n",
    "    dtype=None,\n",
    "):\n",
    "    dataset = XYZFolderDataset(\n",
    "        df=df,\n",
    "        xyz_dir=xyz_dir,\n",
    "        system_config=system_config,\n",
    "        target_col=target_col,\n",
    "        target_config=target_config,\n",
    "        augmentations=augmentations,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    # sampler = RandomSampler(dataset)\n",
    "    # batch_sampler = BatchSampler(sampler, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=utils.worker_init_fn,\n",
    "        collate_fn=collate_graph_and_targets,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "def make_val_loader(\n",
    "    df,\n",
    "    xyz_dir: str,\n",
    "    system_config,\n",
    "    target_col: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    target_config=None,\n",
    "    augmentations=None,\n",
    "    dtype=None,\n",
    "):\n",
    "    dataset = XYZFolderDataset(\n",
    "        df=df,\n",
    "        xyz_dir=xyz_dir,\n",
    "        system_config=system_config,\n",
    "        target_col=target_col,\n",
    "        target_config=target_config,\n",
    "        augmentations=augmentations,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=utils.worker_init_fn,\n",
    "        collate_fn=collate_graph_and_targets,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f7f28-e629-4a8a-b5b8-663278d47393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                reg_drop_rate=0.1,\n",
    "                reg_size=256,\n",
    "                num_labels=1):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        self.reg_drop_rate = reg_drop_rate\n",
    "        self.num_targets = num_labels\n",
    "        self.reg_size = reg_size\n",
    "\n",
    "        # self.gnn = pretrained.orb_v3_direct_inf_mpa(\n",
    "        # device='cpu',\n",
    "        # precision=\"float32-highest\",   # or \"float32-highest\" / \"float64\n",
    "        # )\n",
    "        self.gnn = pretrained.orb_v3_conservative_inf_omat(\n",
    "        device='cpu',\n",
    "        precision=\"float32-highest\",   # or \"float32-highest\" / \"float64\n",
    "        )\n",
    "\n",
    "        self.hidden_size = 256\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(self.reg_drop_rate),\n",
    "            nn.Linear(self.hidden_size, self.reg_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.reg_drop_rate),\n",
    "            nn.Linear(self.reg_size, self.num_targets)\n",
    "        )\n",
    "\n",
    "    def forward(self, batched_graph, layer_idx):\n",
    "        node_emb = self.gnn(batched_graph)\n",
    "        node_emb = node_emb[\"intermediate_layers\"][layer_idx]\n",
    "        batch_index = batched_graph._get_per_node_graph_indices().long()\n",
    "        graph_emb = global_mean_pool(node_emb, batch_index)\n",
    "        output = self.regressor(graph_emb)\n",
    "        return graph_emb, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d17493-0fe2-4a03-97e5-8f801ea27d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eace93e-5876-4df1-89a9-60e9a07170a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "base_dir = '../input_data/tdcommons/admet_group'\n",
    "dfs = []\n",
    "\n",
    "for task in os.listdir(base_dir):\n",
    "    if task.startswith('.'):\n",
    "        continue\n",
    "\n",
    "    print(task)\n",
    "\n",
    "    # if task in skip: \n",
    "    #     print('Skipping ', task)\n",
    "    #     continue\n",
    "\n",
    "    prefix = 'tdcommons/'\n",
    "    if prefix+task in tdc_utils.tdc_mae_tasks:\n",
    "        metric = 'mae'\n",
    "    elif prefix+task in tdc_utils.tdc_spearman_task:\n",
    "        metric = 'spearman'\n",
    "    elif prefix+task in tdc_utils.polaris_pearson_tasks:\n",
    "        metric = 'pearson'\n",
    "    elif prefix+task in tdc_utils.tdc_auroc_tasks:\n",
    "        metric = 'auc'\n",
    "    elif prefix+task in tdc_utils.tdc_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in tdc_utils.tdc_aucpr2_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in tdc_utils.polaris_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    else:\n",
    "        raise ValueError(f\"Task {task} not found in any known task list.\")\n",
    "\n",
    "    try:\n",
    "        data = Tox(name = task)\n",
    "    except:\n",
    "        data = ADME(name = task)\n",
    "\n",
    "    split = data.get_split(method = 'scaffold')\n",
    "\n",
    "    train_df = split['train'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    val_df = split['valid'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    test_df = split['test'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "\n",
    "    if metric in ('mae', 'spearman', 'pearson'):\n",
    "        scaler = StandardScaler()\n",
    "        # fit only on train targets\n",
    "        train_vals = train_df[['target']].values\n",
    "        scaler.fit(train_vals)\n",
    "        # add scaled targets\n",
    "        train_df['target'] = scaler.transform(train_vals)\n",
    "        val_df['target']   = scaler.transform(val_df[['target']].values)\n",
    "        test_df['target']  = scaler.transform(test_df[['target']].values)\n",
    "\n",
    "    for df, split in zip([train_df, val_df, test_df], ['train', 'val', 'test']):\n",
    "        out_dir = f'xyz_files/{task}/{split}/graphs'\n",
    "\n",
    "        # If the folder already exists *and* has at least one .pt file, skip it\n",
    "        if os.path.isdir(out_dir) and any(f.endswith('.xyz') for f in os.listdir(out_dir)):\n",
    "            print('Already exists')\n",
    "            continue\n",
    "\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        for smi in df['smiles'].tolist():\n",
    "            smiles_to_xyz_file(smi, out_dir)\n",
    "\n",
    "    train_dataloader = make_train_loader(\n",
    "        df=train_df,\n",
    "        xyz_dir=f'xyz_files/{task}/train/graphs',\n",
    "        system_config=model.gnn.system_config,\n",
    "        target_col='target',\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        target_config=None,\n",
    "        augmentations=[rotate_randomly],\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    val_dataloader = make_val_loader(\n",
    "        df=val_df,\n",
    "        xyz_dir=f'xyz_files/{task}/val/graphs',\n",
    "        system_config=model.gnn.system_config,\n",
    "        target_col='target',\n",
    "        batch_size=256,\n",
    "        num_workers=1,\n",
    "        target_config=None,\n",
    "        augmentations=None,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    test_dataloader = make_val_loader(\n",
    "        df=test_df,\n",
    "        xyz_dir=f'xyz_files/{task}/test/graphs',\n",
    "        system_config=model.gnn.system_config,\n",
    "        target_col='target',\n",
    "        batch_size=256,\n",
    "        num_workers=1,\n",
    "        target_config=None,\n",
    "        augmentations=None,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    num_layers = 5\n",
    "    layer_indices = list(range(0, num_layers))\n",
    "\n",
    "    test_metrics = []\n",
    "    for layer_idx in layer_indices:\n",
    "        print(f'Layer: {layer_idx}')\n",
    "\n",
    "        accumulation_steps = 1\n",
    "        # hyperparameter sweep over learning rates\n",
    "        for lr in [1e-5, 2e-5, 5e-5, 1e-4, 2e-4]:\n",
    "            print(f'LR={lr}')\n",
    "            model = Model(reg_size=256).to('cuda')\n",
    "            optimizer = AdamW(model.parameters(), lr=lr)\n",
    "            epochs = 50\n",
    "            num_training_steps = len(train_dataloader) * epochs\n",
    "            num_warmup_steps = int(0.05 * num_training_steps)\n",
    "            scheduler_warmup = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "            # choose loss fn by task type\n",
    "            if metric in ('mae', 'spearman', 'pearson'):\n",
    "                loss_fn = MSELoss()\n",
    "            else:\n",
    "                loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "            # train for 100 epochs\n",
    "            # For MAE we want to minimize; for others maximize\n",
    "            if metric == 'mae':\n",
    "                best_val = float('inf')\n",
    "            else:\n",
    "                best_val = -float('inf')\n",
    "            best_state = None\n",
    "            best_epoch = -1\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                for step, (mol_graphs, targets, idxs) in enumerate(train_dataloader):\n",
    "                    mol_graphs = mol_graphs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    _, preds = model(mol_graphs, layer_idx)\n",
    "\n",
    "                    preds_for_loss = preds.squeeze(-1)\n",
    "                    targets_for_loss = targets.squeeze()\n",
    "\n",
    "                    loss = loss_fn(preds_for_loss, targets_for_loss)\n",
    "                    loss = loss / accumulation_steps\n",
    "                    # print(batch)\n",
    "                    # print(preds_for_loss)\n",
    "                    # print(targets_for_loss)\n",
    "                    # print(loss)\n",
    "                    loss.backward()\n",
    "\n",
    "                    # every `accumulation_steps`, do an optimizer step + scheduler step\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        scheduler_warmup.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                # finish off any remaining gradients if dataset size % acc_steps != 0\n",
    "                if (step + 1) % accumulation_steps != 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler_warmup.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # evaluate on validation set\n",
    "                model.eval()\n",
    "                val_preds, val_targs = [], []\n",
    "                with torch.no_grad():\n",
    "                    for mol_graphs, targets, idxs in val_dataloader:\n",
    "                        mol_graphs = mol_graphs.to(device)\n",
    "                        targets = targets.to(device)\n",
    "                        _, preds_tensor = model(mol_graphs, layer_idx)\n",
    "\n",
    "                        current_preds_list = preds_tensor.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "                        current_targs_list = targets.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "\n",
    "                        val_preds.extend(current_preds_list)\n",
    "                        val_targs.extend(current_targs_list)\n",
    "\n",
    "                # compute your chosen metric\n",
    "                if metric == 'mae':\n",
    "                    val_score = np.mean(np.abs(np.array(val_preds) - np.array(val_targs)))\n",
    "                elif metric == 'spearman':\n",
    "                    val_score = spearmanr(val_targs, val_preds)[0]\n",
    "                elif metric == 'pearson':\n",
    "                    val_score = pearsonr(val_targs, val_preds)[0]\n",
    "                elif metric == 'auc':\n",
    "                    val_score = roc_auc_score(val_targs, val_preds)\n",
    "                elif metric == 'aucpr':\n",
    "                    val_score = average_precision_score(val_targs, val_preds)\n",
    "\n",
    "                improved = (metric == 'mae' and val_score < best_val) or (metric != 'mae' and val_score > best_val)\n",
    "                if improved:\n",
    "                    best_val   = val_score\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    best_epoch = epoch\n",
    "                    # print(f\"    ↳ new best val_{metric}: {best_val:.4f} (epoch {best_epoch})\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # now evaluate that best model on the test set\n",
    "        model = Model(reg_size=256).to('cuda')\n",
    "        model.load_state_dict(best_state)\n",
    "        model.eval()\n",
    "        test_preds, test_targs = [], []\n",
    "        with torch.no_grad():\n",
    "            for mol_graphs, targets, idxs in test_dataloader:\n",
    "                mol_graphs = mol_graphs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                _, preds_tensor = model(mol_graphs, layer_idx)\n",
    "\n",
    "                current_preds_list = preds_tensor.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "                current_targs_list = targets.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "\n",
    "                test_preds.extend(current_preds_list)\n",
    "                test_targs.extend(current_targs_list)\n",
    "\n",
    "        if metric in ('mae', 'spearman', 'pearson'):\n",
    "            # bring preds back to original units\n",
    "            test_preds = scaler.inverse_transform(\n",
    "                np.array(test_preds).reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            # use original test targets (unscaled)\n",
    "            test_targs = scaler.inverse_transform(\n",
    "                np.array(test_targs).reshape(-1, 1)\n",
    "            ).flatten()\n",
    "\n",
    "        if metric == 'mae':\n",
    "            test_score = np.mean(np.abs(np.array(test_preds) - np.array(test_targs)))\n",
    "        elif metric == 'spearman':\n",
    "            test_score = spearmanr(test_targs, test_preds)[0]\n",
    "        elif metric == 'pearson':\n",
    "            test_score = pearsonr(test_targs, test_preds)[0]\n",
    "        elif metric == 'auc':\n",
    "            test_score = roc_auc_score(test_targs, test_preds)\n",
    "        elif metric == 'aucpr':\n",
    "            test_score = average_precision_score(test_targs, test_preds)\n",
    "\n",
    "        test_metrics.append(test_score)\n",
    "\n",
    "        print(f\"Layer {layer_idx}: {test_score}\")\n",
    "\n",
    "    # save a DataFrame with one column per task and rows = layer indices\n",
    "    results_df = pd.DataFrame({task: test_metrics}, index=layer_indices)\n",
    "    dfs.append(results_df)\n",
    "    results_df.to_csv(f\"tmp/orb_conserv_{task}_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e618e-4db4-4c57-8f1b-be04cbe1599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for df in os.listdir('tmp_256'):\n",
    "    if 'conserv' in df:\n",
    "        dfs.append(pd.read_csv('tmp_256/'+df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9928084-b40a-4043-9e9d-073e932be0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(dfs, axis=1).to_csv('./results_orb_conserv_finetune.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76bc24-8e21-4506-b0be-738317856ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_mask = torch.isnan(preds)\n",
    "# if nan_mask.any():\n",
    "#     bad_positions = nan_mask.nonzero(as_tuple=True)[0]\n",
    "#     print(\"⚠️ NaN in preds at batch indices:\", bad_positions.tolist())\n",
    "    # batch.idx gives you the original dataset index for each sample\n",
    "    # bad_dataset_idxs = batch.idx[bad_positions].tolist()\n",
    "    # print(\"… which correspond to dataset indices:\", bad_dataset_idxs)\n",
    "    # for di in bad_dataset_idxs:\n",
    "    #     print(\"  file:\", train_dataset.paths[di])\n",
    "    # raise RuntimeError(\"Stopping: found NaN in model output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099c0f4e-72c2-439c-8bfd-c95d590e6f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for step, (mol_graphs, targets, idxs) in enumerate(train_dataloader):\n",
    "#     mol_graphs = mol_graphs.to('cuda')\n",
    "#     idxs = idxs.to('cuda')\n",
    "#     with torch.no_grad():\n",
    "#         _, preds = model(mol_graphs, layer_idx)\n",
    "#     nan_mask = torch.isnan(preds)\n",
    "#     if nan_mask.any():\n",
    "#         bad_batch_pos = nan_mask.nonzero(as_tuple=True)[0].unique()\n",
    "#         bad_dataset_idx = idxs[bad_batch_pos].tolist()\n",
    "#         print(\"⚠️ NaN preds at batch positions\", bad_batch_pos.tolist(),\n",
    "#               \"→ dataset rows\", bad_dataset_idx)\n",
    "#         # now inspect each bad graph\n",
    "#         for di in bad_dataset_idx:\n",
    "#             g, t, _ = train_dataloader.dataset[di]\n",
    "#             print(f\"\\n--- Inspecting dataset row {di}:\")\n",
    "#             print(train_dataloader.dataset.df.iloc[di])\n",
    "#             # check graph-level stats\n",
    "#             print(f\"  n_nodes = {g.n_node}, n_edges = {g.n_edge}\")\n",
    "#             # assume node features live in g.node_features\n",
    "#             nf = g.node_features\n",
    "#             ef = g.edge_features   # adjust to your field names\n",
    "#             print(\"  node_features has NaN?\", torch.isnan(nf).any().item())\n",
    "#             print(\"  edge_features has NaN?\", torch.isnan(ef).any().item())\n",
    "#             # if you store positions on the graph:\n",
    "#             pos = g.node_positions\n",
    "#             print(\"  positions has NaN?\", torch.isnan(pos).any().item())\n",
    "#         break  # stop after first failure to keep logs manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea86286-a898-4315-90b9-804e083c9db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# pos = g.node_features[\"positions\"]  # shape: (n_nodes, 3)\n",
    "# # Compute full pairwise distance matrix\n",
    "# # (PyTorch ≥1.1 has torch.cdist; if you’re on older PyTorch, you can do it with broadcasting.)\n",
    "# dists = torch.cdist(pos, pos)  # shape: (n_nodes, n_nodes)\n",
    "\n",
    "# # Zero out the diagonal so we only look at *distinct* atom pairs\n",
    "# n = pos.size(0)\n",
    "# eye = torch.eye(n, dtype=torch.bool, device=dists.device)\n",
    "# dists[eye] = float(\"inf\")\n",
    "\n",
    "# # Find all pairs closer than eps (e.g. exactly zero or nearly zero)\n",
    "# eps = 1e-6\n",
    "# overlap_mask = dists <= eps\n",
    "# overlaps = overlap_mask.nonzero(as_tuple=False)  # each row is [i, j]\n",
    "\n",
    "# if overlaps.numel() == 0:\n",
    "#     print(\"✔️ No overlapping atoms detected.\")\n",
    "# else:\n",
    "#     print(\"⚠️ Overlapping atom pairs (i, j):\")\n",
    "#     for i, j in overlaps.tolist():\n",
    "#         print(f\"   Atom {i} and Atom {j} both at {pos[i].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404aa78-1f58-4758-9b93-62f63548cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assume g is the single-graph AtomGraphs you pulled out for idx 6128\n",
    "\n",
    "# print(f\"\\nGraph has {g.n_node.item()} nodes, {g.n_edge.item()} edges\\n\")\n",
    "\n",
    "# # 1) Check every node-feature tensor\n",
    "# for name, feat in g.node_features.items():\n",
    "#     has_nan = torch.isnan(feat).any().item()\n",
    "#     print(f\"node_feature[{name}]: shape={tuple(feat.shape)}, NaN? {has_nan}\")\n",
    "#     if has_nan:\n",
    "#         print(\"   →\", name, \"min/max:\", feat.nanmin().item(), feat.nanmax().item())\n",
    "\n",
    "# # 2) Check every edge-feature tensor\n",
    "# for name, feat in g.edge_features.items():\n",
    "#     has_nan = torch.isnan(feat).any().item()\n",
    "#     print(f\"edge_feature[{name}]: shape={tuple(feat.shape)}, NaN? {has_nan}\")\n",
    "#     if has_nan:\n",
    "#         print(\"   →\", name, \"min/max:\", feat.nanmin().item(), feat.nanmax().item())\n",
    "\n",
    "# # 3) If you store positions explicitly on g:\n",
    "# if hasattr(g, \"node_positions\"):\n",
    "#     pos = g.node_positions\n",
    "#     print(\"node_positions:\", pos.shape, \"NaN?\", torch.isnan(pos).any().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a79b74-3e87-4c53-8724-b168ffd6fc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101628e6-0b8f-40ed-888f-a7c4f5d8eda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d3e06-2b66-4fa5-bc9d-980769317f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ca7a7-302a-4dc3-bb93-4deb989db317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
