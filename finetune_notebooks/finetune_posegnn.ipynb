{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install --upgrade torch_geometric\n",
    "# ! pip install ase==3.24.0\n",
    "# ! pip install torch_nl==0.3\n",
    "# ! pip install rdkit\n",
    "# ! pip install PyTDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import glob, copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import MSELoss, BCEWithLogitsLoss\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from ase import Atoms as ASEAtoms\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "from tdc.single_pred import ADME, Tox\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../external_repos/')\n",
    "from posegnn.model import PosEGNN\n",
    "\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_ase_atoms(smiles: str, random_seed: int = 42,\n",
    "                        jitter_amp: float = 1e-2) -> ASEAtoms | None:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    mol = Chem.AddHs(mol)\n",
    "    \n",
    "    # Try different embedding methods in order of preference\n",
    "    conformer_generated = False\n",
    "    \n",
    "    # Method 1: Standard embedding with UFF optimization\n",
    "    try:\n",
    "        embed_result = AllChem.EmbedMolecule(mol, maxAttempts=2000, randomSeed=random_seed, useRandomCoords=True)\n",
    "        if embed_result == 0:  # Success\n",
    "            # UFF optimization\n",
    "            uff_props = AllChem.UFFGetMoleculeForceField(mol)\n",
    "            if uff_props is not None:\n",
    "                uff_props.Initialize()\n",
    "                uff_props.Minimize(maxIts=1000)\n",
    "                \n",
    "                # Add jitter to avoid symmetry issues\n",
    "                conf = mol.GetConformer()\n",
    "                pos = conf.GetPositions()\n",
    "                np.random.seed(random_seed)\n",
    "                noise = (np.random.rand(*pos.shape) - 0.5) * jitter_amp\n",
    "                new_pos = pos + noise\n",
    "                for i, p in enumerate(new_pos):\n",
    "                    conf.SetAtomPosition(i, p.tolist())\n",
    "                \n",
    "                # Final MMFF optimization\n",
    "                AllChem.MMFFOptimizeMolecule(mol)\n",
    "                conformer_generated = True\n",
    "    except Exception as e:\n",
    "        print(f\"Method 1 failed: {e}\")\n",
    "        pass\n",
    "    \n",
    "    # Method 2: Try ETKDGv3 if method 1 failed\n",
    "    if not conformer_generated:\n",
    "        try:\n",
    "            embed_result = AllChem.EmbedMolecule(mol, AllChem.ETKDGv3())\n",
    "            if embed_result == 0:\n",
    "                conformer_generated = True\n",
    "        except Exception as e:\n",
    "            print(f\"Method 2 failed: {e}\")\n",
    "            pass\n",
    "    \n",
    "    # Method 3: Try explicit conformer creation with 2D coords\n",
    "    if not conformer_generated:\n",
    "        try:\n",
    "            # Create a fresh conformer explicitly\n",
    "            conf = Chem.Conformer(mol.GetNumAtoms())\n",
    "            for i in range(mol.GetNumAtoms()):\n",
    "                conf.SetAtomPosition(i, [0.0, 0.0, 0.0])  # Initialize with zeros\n",
    "            conf_id = mol.AddConformer(conf)\n",
    "            \n",
    "            # Now compute 2D coords\n",
    "            AllChem.Compute2DCoords(mol)\n",
    "            \n",
    "            # Check if the conformer exists and has non-zero coordinates\n",
    "            if mol.GetNumConformers() > 0:\n",
    "                conformer_generated = True\n",
    "        except Exception as e:\n",
    "            print(f\"Method 3 failed: {e}\")\n",
    "            pass\n",
    "\n",
    "    # Bail out if we couldn't generate any conformer\n",
    "    if not conformer_generated or mol.GetNumConformers() == 0:\n",
    "        return None\n",
    "\n",
    "    # Get the conformer\n",
    "    conf = mol.GetConformer()\n",
    "\n",
    "    def has_overlaps(pos, eps=1e-6):\n",
    "        d = torch.cdist(torch.tensor(pos), torch.tensor(pos))\n",
    "        n = d.shape[0]\n",
    "        d[range(n), range(n)] = float('inf')\n",
    "        return (d <= eps).any().item()\n",
    "\n",
    "    pos = conf.GetPositions()\n",
    "    if has_overlaps(pos, eps=1e-6):\n",
    "        # you could loop a few times here with new seeds,\n",
    "        # or simply bail out and let your dataset drop it:\n",
    "        return None\n",
    "    \n",
    "    # Extract heavy atoms only\n",
    "    heavy_atoms = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        if atom.GetSymbol() != 'H':\n",
    "            idx = atom.GetIdx()\n",
    "            try:\n",
    "                pos = conf.GetAtomPosition(idx)\n",
    "                heavy_atoms.append((atom.GetSymbol(), pos))\n",
    "            except Exception:\n",
    "                # Skip this atom if position can't be retrieved\n",
    "                continue\n",
    "\n",
    "    # If no heavy atoms were successfully processed, fail\n",
    "    if not heavy_atoms:\n",
    "        return None\n",
    "\n",
    "    # Final check\n",
    "    mol = Chem.RemoveHs(mol)\n",
    "    conf = mol.GetConformer()\n",
    "    pos = conf.GetPositions()\n",
    "    # if np.allclose(pos, 0) or np.allclose(pos[:, 2], 0) or np.isnan(pos).any():\n",
    "    #     return None\n",
    "\n",
    "    return ASEAtoms(\n",
    "        symbols=[a.GetSymbol() for a in mol.GetAtoms()],\n",
    "        positions=pos,\n",
    "        pbc=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_from_ase(atoms: ASEAtoms) -> Data:\n",
    "    z = torch.tensor(atoms.get_atomic_numbers(), dtype=torch.long)\n",
    "    box = torch.tensor(atoms.get_cell().tolist()).unsqueeze(0).float()\n",
    "    pos = torch.tensor(atoms.get_positions().tolist()).float()\n",
    "    batch = torch.zeros(len(z), dtype=torch.long)\n",
    "    return Data(z=z, pos=pos, box=box, batch=batch, num_graphs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileBacked3DDataset(Dataset):\n",
    "    def __init__(self, root_folder: str):\n",
    "        # collects e.g. ['.../0.pt','.../1.pt', ...]\n",
    "        self.paths = sorted(glob.glob(f'{root_folder}/*.pt'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Data:\n",
    "        # load just one graph\n",
    "        data: Data = torch.load(self.paths[idx])\n",
    "        data.idx = idx\n",
    "        # Ensure all tensors are on CPU and detached\n",
    "        for key, value in data:\n",
    "            if isinstance(value, torch.Tensor) and torch.isnan(value).any():\n",
    "                raise ValueError(f\"üõë NaN detected in '{key}'\")\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                data[key] = value.cpu().detach()\n",
    "        return data\n",
    "\n",
    "def collate_fn(batch_list):\n",
    "    return Batch.from_data_list(batch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                reg_drop_rate=0.1,\n",
    "                intermediate_size=256,\n",
    "                num_targets=1):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        self.reg_drop_rate = reg_drop_rate\n",
    "        self.num_targets = num_targets\n",
    "        self.intermediate_size = intermediate_size\n",
    "\n",
    "        self.hidden_size = 256\n",
    "        checkpoint_dict = torch.load('pytorch_model.bin', weights_only=True, map_location=device)\n",
    "        self.transformer = PosEGNN(checkpoint_dict[\"config\"])\n",
    "        self.transformer.load_state_dict(checkpoint_dict[\"state_dict\"], strict=False)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(self.reg_drop_rate),\n",
    "            nn.Linear(self.hidden_size, self.intermediate_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.reg_drop_rate),\n",
    "            nn.Linear(self.intermediate_size, self.num_targets)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch, layer_idx=-1):\n",
    "        output_dict = self.transformer(batch)\n",
    "        node_emb = output_dict[\"embedding_0\"]\n",
    "        if node_emb.dim() == 3:\n",
    "            node_emb = node_emb[:, :, layer_idx]  # -> [total_nodes, hidden_dim]\n",
    "        # `batch.batch` is the graph‚Äêindex for each node\n",
    "        graph_emb = global_mean_pool(node_emb, batch.batch) # -> [batch_size, hidden_dim]\n",
    "        return self.regressor(graph_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['caco2_wang', 'clearance_hepatocyte_az',\n",
    "         'cyp2c9_substrate_carbonmangels', 'cyp3a4_substrate_carbonmangels',\n",
    "         'cyp3a4_veith', 'dili', 'half_life_obach',\n",
    "         'herg', 'ld50_zhu', 'lipophilicity_astrazeneca',\n",
    "         'ppbr_az', 'solubility_aqsoldb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "base_dir = 'tdcommons/admet_group'\n",
    "dfs = []\n",
    "\n",
    "for task in tasks:\n",
    "    if task.startswith('.'):\n",
    "        continue\n",
    "\n",
    "    print(task)\n",
    "    \n",
    "    # if task in ['cyp2d6_veith', 'hia_hou']: continue\n",
    "\n",
    "    prefix = 'tdcommons/'\n",
    "    if prefix+task in utils.tdc_mae_tasks:\n",
    "        metric = 'mae'\n",
    "    elif prefix+task in utils.tdc_spearman_task:\n",
    "        metric = 'spearman'\n",
    "    elif prefix+task in utils.polaris_pearson_tasks:\n",
    "        metric = 'pearson'\n",
    "    elif prefix+task in utils.tdc_auroc_tasks:\n",
    "        metric = 'auc'\n",
    "    elif prefix+task in utils.tdc_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in utils.tdc_aucpr2_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in utils.polaris_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    else:\n",
    "        raise ValueError(f\"Task {task} not found in any known task list.\")\n",
    "\n",
    "    try:\n",
    "        data = ADME(name = task)\n",
    "    except:\n",
    "        data = Tox(name = task)\n",
    "\n",
    "    split = data.get_split(method = 'scaffold')\n",
    "\n",
    "    train_df = split['train'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    val_df = split['valid'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    test_df = split['test'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "\n",
    "    if metric in ('mae', 'spearman', 'pearson'):\n",
    "        scaler = StandardScaler()\n",
    "        # fit only on train targets\n",
    "        train_vals = train_df[['target']].values\n",
    "        scaler.fit(train_vals)\n",
    "        # add scaled targets\n",
    "        train_df['target'] = scaler.transform(train_vals)\n",
    "        val_df['target']   = scaler.transform(val_df[['target']].values)\n",
    "        test_df['target']  = scaler.transform(test_df[['target']].values)\n",
    "\n",
    "    for df, split in zip([train_df, val_df, test_df], ['train','val','test']):\n",
    "        out_dir = f'3D_data/{task}/{split}/graphs'\n",
    "\n",
    "        # If the folder already exists *and* has at least one .pt file, skip it\n",
    "        if os.path.isdir(out_dir) and any(f.endswith('.pt') for f in os.listdir(out_dir)):\n",
    "            continue\n",
    "\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            atoms = smiles_to_ase_atoms(row.smiles)\n",
    "            if atoms is None:\n",
    "                continue\n",
    "            data = build_data_from_ase(atoms) \n",
    "            assert data.num_nodes > 0, f\"no atoms in idx={idx}\"\n",
    "            data.y = torch.tensor([row.target], dtype=torch.float32)\n",
    "\n",
    "            if torch.isnan(data.pos).any() or torch.isinf(data.pos).any():\n",
    "                print(f\"WARNING: NaN or Inf in positions for SMILES: {row.smiles}, idx: {idx}\")\n",
    "                # Decide how to handle: skip, log, etc.\n",
    "                continue\n",
    "            if torch.isnan(data.y).any() or torch.isinf(data.y).any():\n",
    "                print(f\"WARNING: NaN or Inf in target for SMILES: {row.smiles}, idx: {idx}\")\n",
    "                # This might indicate issues with the scaler or original data\n",
    "                continue\n",
    "\n",
    "            if not torch.isfinite(data.y).all():\n",
    "                print(f\"Skipping idx={idx} due to NaN target\")\n",
    "                continue\n",
    "\n",
    "            torch.save(data, os.path.join(out_dir, f'{idx}.pt'))\n",
    "        print(f\"‚úî Finished dumping {len(os.listdir(out_dir))} graphs for {task}/{split}\")\n",
    "\n",
    "    train_dataset = FileBacked3DDataset(f'3D_data/{task}/train/graphs')\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    val_dataset = FileBacked3DDataset(f'3D_data/{task}/val/graphs')\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    test_dataset = FileBacked3DDataset(f'3D_data/{task}/test/graphs')\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    num_layers = 4\n",
    "    layer_indices = list(range(0, num_layers))\n",
    "\n",
    "    test_metrics = []\n",
    "    for layer_idx in layer_indices:\n",
    "        print(f'Layer: {layer_idx}')\n",
    "\n",
    "        accumulation_steps = 1\n",
    "        # hyperparameter sweep over learning rates\n",
    "        for lr in [1e-5, 2e-5, 5e-5, 1e-4, 2e-4]:\n",
    "            print(f'LR={lr}')\n",
    "            model = Transformer().to('cuda')\n",
    "            optimizer = AdamW(model.parameters(), lr=lr)\n",
    "            epochs = 50\n",
    "            num_training_steps = len(train_dataloader) * epochs\n",
    "            num_warmup_steps = int(0.05 * num_training_steps)\n",
    "            scheduler_warmup = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "            # choose loss fn by task type\n",
    "            if metric in ('mae', 'spearman', 'pearson'):\n",
    "                loss_fn = MSELoss()\n",
    "            else:\n",
    "                loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "            # train for 100 epochs\n",
    "            # For MAE we want to minimize; for others maximize\n",
    "            if metric == 'mae':\n",
    "                best_val = float('inf')\n",
    "            else:\n",
    "                best_val = -float('inf')\n",
    "            best_state = None\n",
    "            best_epoch = -1\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                for step, batch in enumerate(train_dataloader):\n",
    "                    if torch.isnan(batch.pos).any() or torch.isnan(batch.y).any():\n",
    "                        print(f\"NaN detected in input at step {step}\")\n",
    "                        print(batch.batch)\n",
    "                        raise RuntimeError(\"Stopping due to NaN in batch\")\n",
    "                    optimizer.zero_grad()\n",
    "                    preds = model(batch.to(device), layer_idx)\n",
    "                    targets = batch.y.squeeze().to(device)\n",
    "\n",
    "                    preds_for_loss = preds.squeeze(-1)\n",
    "                    targets_for_loss = targets.squeeze()\n",
    "\n",
    "                    loss = loss_fn(preds_for_loss, targets_for_loss)\n",
    "                    loss = loss / accumulation_steps\n",
    "                    # print(batch)\n",
    "                    # print(preds_for_loss)\n",
    "                    # print(targets_for_loss)\n",
    "                    # print(loss)\n",
    "                    loss.backward()\n",
    "\n",
    "                    # every `accumulation_steps`, do an optimizer step + scheduler step\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        scheduler_warmup.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                # finish off any remaining gradients if dataset size % acc_steps != 0\n",
    "                if (step + 1) % accumulation_steps != 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler_warmup.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # evaluate on validation set\n",
    "                model.eval()\n",
    "                val_preds, val_targs = [], []\n",
    "                with torch.no_grad():\n",
    "                    for batch in val_dataloader:\n",
    "                        batch_on_device = batch.to(device)\n",
    "                        preds_tensor = model(batch_on_device, layer_idx)\n",
    "    \n",
    "                        current_preds_list = preds_tensor.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "                        targs_tensor = batch_on_device.y # Targets from the batch on device\n",
    "                        current_targs_list = targs_tensor.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "    \n",
    "                        val_preds.extend(current_preds_list)\n",
    "                        val_targs.extend(current_targs_list)\n",
    "    \n",
    "                # compute your chosen metric\n",
    "                if metric == 'mae':\n",
    "                    val_score = np.mean(np.abs(np.array(val_preds) - np.array(val_targs)))\n",
    "                elif metric == 'spearman':\n",
    "                    val_score = spearmanr(val_targs, val_preds)[0]\n",
    "                elif metric == 'pearson':\n",
    "                    val_score = pearsonr(val_targs, val_preds)[0]\n",
    "                elif metric == 'auc':\n",
    "                    val_score = roc_auc_score(val_targs, val_preds)\n",
    "                elif metric == 'aucpr':\n",
    "                    val_score = average_precision_score(val_targs, val_preds)\n",
    "    \n",
    "                improved = (metric == 'mae' and val_score < best_val) or (metric != 'mae' and val_score > best_val)\n",
    "                if improved:\n",
    "                    best_val   = val_score\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    best_epoch = epoch\n",
    "                    # print(f\"    ‚Ü≥ new best val_{metric}: {best_val:.4f} (epoch {best_epoch})\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # now evaluate that best model on the test set\n",
    "        model = Transformer().to('cuda')\n",
    "        model.load_state_dict(best_state)\n",
    "        model.eval()\n",
    "        test_preds, test_targs = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                batch_on_device = batch.to(device)\n",
    "                preds_tensor = model(batch_on_device, layer_idx)\n",
    "\n",
    "                current_preds_list = preds_tensor.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "                targs_tensor = batch_on_device.y # Targets from the batch on device\n",
    "                current_targs_list = targs_tensor.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "\n",
    "                test_preds.extend(current_preds_list)\n",
    "                test_targs.extend(current_targs_list)\n",
    "\n",
    "        if metric in ('mae', 'spearman', 'pearson'):\n",
    "            # bring preds back to original units\n",
    "            test_preds = scaler.inverse_transform(\n",
    "                np.array(test_preds).reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            # use original test targets (unscaled)\n",
    "            test_targs = scaler.inverse_transform(\n",
    "                np.array(test_targs).reshape(-1, 1)\n",
    "            ).flatten()\n",
    "\n",
    "        if metric == 'mae':\n",
    "            test_score = np.mean(np.abs(np.array(test_preds) - np.array(test_targs)))\n",
    "        elif metric == 'spearman':\n",
    "            test_score = spearmanr(test_targs, test_preds)[0]\n",
    "        elif metric == 'pearson':\n",
    "            test_score = pearsonr(test_targs, test_preds)[0]\n",
    "        elif metric == 'auc':\n",
    "            test_score = roc_auc_score(test_targs, test_preds)\n",
    "        elif metric == 'aucpr':\n",
    "            test_score = average_precision_score(test_targs, test_preds)\n",
    "\n",
    "        test_metrics.append(test_score)\n",
    "\n",
    "        print(f\"Layer {layer_idx}: {test_score}\")\n",
    "\n",
    "    # save a DataFrame with one column per task and rows = layer indices\n",
    "    results_df = pd.DataFrame({task: test_metrics}, index=layer_indices)\n",
    "    dfs.append(results_df)\n",
    "    results_df.to_csv(f\"tmp/posegnn_{task}_layer_results.csv\", index=False)\n",
    "\n",
    "# dfs = pd.concat(dfs, axis=1)\n",
    "# dfs.to_csv('./results_posegnn_finetune.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model(batch.to(device), layer_idx)        # -> [B,1]\n",
    "# preds = preds.squeeze(-1)                         # -> [B]\n",
    "\n",
    "# nan_mask = torch.isnan(preds)\n",
    "# if nan_mask.any():\n",
    "#     bad_positions = nan_mask.nonzero(as_tuple=True)[0]\n",
    "#     print(\"‚ö†Ô∏è NaN in preds at batch indices:\", bad_positions.tolist())\n",
    "#     # batch.idx gives you the original dataset index for each sample\n",
    "#     bad_dataset_idxs = batch.idx[bad_positions].tolist()\n",
    "#     print(\"‚Ä¶ which correspond to dataset indices:\", bad_dataset_idxs)\n",
    "#     for di in bad_dataset_idxs:\n",
    "#         print(\"  file:\", train_dataset.paths[di])\n",
    "#     raise RuntimeError(\"Stopping: found NaN in model output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
