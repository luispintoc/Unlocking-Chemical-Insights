{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2fc01d-75b6-4754-863d-b3e6f13b18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers==4.46.3\n",
    "# ! pip install --upgrade torch_geometric\n",
    "# ! pip install torch_nl==0.3\n",
    "# ! pip install PyTDC\n",
    "# ! pip install rdkit==2023.9.4\n",
    "# ! pip install addict\n",
    "# ! pip install ase==3.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea77c81-dd66-4f03-8e88-aea6c3a0617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import ase\n",
    "from ase.io import read\n",
    "from ase import Atoms\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import MSELoss, BCEWithLogitsLoss\n",
    "from torch import distributed as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "# allow imports from parent directory\n",
    "# sys.path.append(os.path.abspath(os.path.join(__file__, os.pardir, os.pardir)))\n",
    "import sys\n",
    "sys.path.append('../external_repos/')\n",
    "from UniMol.unimol_tools.unimol_tools.models import UniMolModel\n",
    "from UniMol.unimol_tools.unimol_tools.data.conformer import coords2unimol\n",
    "from UniMol.unimol_tools.unimol_tools.data.dictionary import Dictionary\n",
    "from tdc.benchmark_group import admet_group\n",
    "from tdc.single_pred import ADME, Tox\n",
    "\n",
    "sys.path.append('../')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a37aa7-558c-4494-a7b0-a228025ce18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary().load('unimol.dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf48ecf-4920-4eb8-b040-e572e31eb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smiles(input_file):\n",
    "    # Read the entire content of the XYZ file.\n",
    "    with open(input_file, 'r') as f:\n",
    "        xyz_data = f.read()\n",
    "\n",
    "    # Split the content into lines.\n",
    "    lines = xyz_data.splitlines()\n",
    "    smiles = lines[1].strip()\n",
    "\n",
    "    return smiles\n",
    "\n",
    "class XYZFolderDataset():\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,                        # your pandas DataFrame\n",
    "        xyz_dir: Union[str, Path],\n",
    "        target_col: str,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.xyz_dir = Path(xyz_dir)\n",
    "        self.target_col = target_col\n",
    "        self.dtype = dtype or torch.get_default_dtype()\n",
    "\n",
    "        # build smiles → path lookup\n",
    "        self.xyz_dir = Path(xyz_dir)\n",
    "        lookup = {}\n",
    "        for p in self.xyz_dir.glob(\"*.xyz\"):\n",
    "            s = get_smiles(str(p))\n",
    "            lookup[s] = p\n",
    "        self._lookup = lookup\n",
    "\n",
    "        df = df.reset_index(drop=True)\n",
    "        mask = df['smiles'].isin(lookup)\n",
    "        num_bad = (~mask).sum()\n",
    "        if num_bad > 0:\n",
    "            print(f\"Warning: dropping {num_bad} rows with no matching .xyz file\")\n",
    "        self.df = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        smiles = row['smiles']\n",
    "        target = row[self.target_col]\n",
    "\n",
    "        xyz_path = self._lookup.get(smiles)\n",
    "        if xyz_path is None:\n",
    "            raise KeyError(f\"Could not find .xyz for SMILES `{smiles}` in {self.xyz_dir}\")\n",
    "\n",
    "        # read Atoms\n",
    "        xyz = ase.io.read(str(xyz_path))\n",
    "        # extract targets into atoms.info\n",
    "        atoms = xyz.get_chemical_symbols()\n",
    "        coords = xyz.get_positions()\n",
    "\n",
    "        # to graph\n",
    "        graph = coords2unimol(atoms, coords, dictionary)\n",
    "\n",
    "        # return graph + scalar target\n",
    "        return graph, torch.tensor(target, dtype=self.dtype)\n",
    "\n",
    "    def get_atom(self, idx: int) -> ase.Atoms:\n",
    "        \"\"\"Return the raw ASE Atoms for this index.\"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "        xyz_path = self._lookup[row['smiles']]\n",
    "        return ase.io.read(str(xyz_path))\n",
    "\n",
    "    def get_atom_and_metadata(self, idx: int) -> Tuple[ase.Atoms, Dict]:\n",
    "        \"\"\"Return both ASE Atoms and the row’s metadata dict.\"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "        xyz_path = self._lookup[row['smiles']]\n",
    "        atoms = ase.io.read(str(xyz_path))\n",
    "        return atoms, row.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db646f1-8d67-44de-b896-9c8071af4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_graph_and_targets(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (graph_dict, target_tensor)\n",
    "      - graph_dict keys are NumPy arrays:\n",
    "          'src_tokens'    -> [N]\n",
    "          'src_distance'  -> [N, N]\n",
    "          'src_coord'     -> [N, 3]\n",
    "          'src_edge_type' -> [N, N]\n",
    "      - target_tensor: torch.Tensor scalar\n",
    "\n",
    "    Returns:\n",
    "      batched_graph: dict of torch.Tensor with shapes\n",
    "           'src_tokens':    [B, N_max]\n",
    "           'src_distance':  [B, N_max, N_max]\n",
    "           'src_coord':     [B, N_max, 3]\n",
    "           'src_edge_type': [B, N_max, N_max]\n",
    "      batched_targets: torch.Tensor [B]\n",
    "    \"\"\"\n",
    "    graphs, targets = zip(*batch)\n",
    "    # Determine max number of nodes in this batch\n",
    "    max_nodes = max(g['src_tokens'].shape[0] for g in graphs)\n",
    "    pad_id = dictionary.pad()  # assume dictionary is in scope\n",
    "\n",
    "    padded_tokens = []\n",
    "    padded_dist   = []\n",
    "    padded_coord  = []\n",
    "    padded_edge   = []\n",
    "\n",
    "    for g in graphs:\n",
    "        # convert to torch tensors\n",
    "        tokens = torch.as_tensor(g['src_tokens'],    dtype=torch.long)\n",
    "        dist   = torch.as_tensor(g['src_distance'],  dtype=torch.float)\n",
    "        coord  = torch.as_tensor(g['src_coord'],     dtype=torch.float)\n",
    "        edge   = torch.as_tensor(g['src_edge_type'], dtype=torch.long)\n",
    "\n",
    "        n = tokens.size(0)\n",
    "        pad_len = max_nodes - n\n",
    "\n",
    "        # 1D pad: (pad_left, pad_right)\n",
    "        tokens_p = F.pad(tokens, (0, pad_len), value=pad_id)\n",
    "        # 2D pad: (last_dim_left, last_dim_right, second_last_left, second_last_right)\n",
    "        dist_p   = F.pad(dist,   (0, pad_len, 0, pad_len), value=0.0)\n",
    "        coord_p  = F.pad(coord,  (0, 0,       0, pad_len), value=0.0)\n",
    "        edge_p   = F.pad(edge,   (0, pad_len, 0, pad_len), value=pad_id)\n",
    "\n",
    "        padded_tokens.append(tokens_p)\n",
    "        padded_dist.append(dist_p)\n",
    "        padded_coord.append(coord_p)\n",
    "        padded_edge.append(edge_p)\n",
    "\n",
    "    batched_graph = {\n",
    "        'src_tokens':    torch.stack(padded_tokens, dim=0),\n",
    "        'src_distance':  torch.stack(padded_dist,     dim=0),\n",
    "        'src_coord':     torch.stack(padded_coord,    dim=0),\n",
    "        'src_edge_type': torch.stack(padded_edge,     dim=0),\n",
    "    }\n",
    "    batched_targets = torch.stack(targets, dim=0)\n",
    "\n",
    "    return batched_graph, batched_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0bc00-0414-4ff1-97ff-b74ba9fe45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(\n",
    "    df,\n",
    "    xyz_dir: str,\n",
    "    target_col: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    shuffle:  bool,\n",
    "    dtype=None,\n",
    "):\n",
    "    dataset = XYZFolderDataset(\n",
    "        df=df,\n",
    "        xyz_dir=xyz_dir,\n",
    "        target_col=target_col,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_graph_and_targets,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d7707-9e1a-45ec-ae01-24339a8333e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                reg_drop_rate=0.1,\n",
    "                reg_size=256,\n",
    "                num_labels=1):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        self.reg_drop_rate = reg_drop_rate\n",
    "        self.num_targets = num_labels\n",
    "        self.reg_size = reg_size\n",
    "\n",
    "        self.gnn = UniMolModel(remove_hs=True)\n",
    "\n",
    "        self.hidden_size = 512\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(self.reg_drop_rate),\n",
    "            nn.Linear(self.hidden_size, self.reg_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(self.reg_drop_rate),\n",
    "            nn.Linear(self.reg_size, self.num_targets)\n",
    "        )\n",
    "\n",
    "    def forward(self, batched_graph, layer_idx):\n",
    "        node_emb = self.gnn(**batched_graph, layer_idx=layer_idx, return_repr=True)\n",
    "        node_emb = node_emb[\"intermediate_layer_reprs\"][-1]\n",
    "        graph_emb = node_emb[:, 0, :]\n",
    "        output = self.regressor(graph_emb)\n",
    "        return graph_emb, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5897db-d80c-485e-b54e-4f16d61376bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype  = torch.float32\n",
    "compile = None\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32  = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbadedb5-da64-41a3-be17-7c8654b715de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d800a-4c68-47ba-9881-bfc3ef82d760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(batch, device):\n",
    "    if hasattr(batch, 'to'):\n",
    "        return batch.to(device)\n",
    "    elif isinstance(batch, dict):\n",
    "        return {k: move_to_device(v, device) for k, v in batch.items()}\n",
    "    elif isinstance(batch, (list, tuple)):\n",
    "        return type(batch)(move_to_device(v, device) for v in batch)\n",
    "    elif isinstance(batch, torch.Tensor):\n",
    "        return batch.to(device)\n",
    "    else:\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683e714-3776-4afa-be6b-2718732fd676",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['solubility_aqsoldb'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a77f86-1c13-43f5-b6ba-153518d9d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "base_dir = 'tdcommons/admet_group'\n",
    "dfs = []\n",
    "\n",
    "for task in tasks:\n",
    "    if task.startswith('.'):\n",
    "        continue\n",
    "\n",
    "    print(task)\n",
    "\n",
    "    # if task in skip: continue\n",
    "\n",
    "    prefix = 'tdcommons/'\n",
    "    if prefix+task in utils.tdc_mae_tasks:\n",
    "        metric = 'mae'\n",
    "    elif prefix+task in utils.tdc_spearman_task:\n",
    "        metric = 'spearman'\n",
    "    elif prefix+task in utils.polaris_pearson_tasks:\n",
    "        metric = 'pearson'\n",
    "    elif prefix+task in utils.tdc_auroc_tasks:\n",
    "        metric = 'auc'\n",
    "    elif prefix+task in utils.tdc_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in utils.tdc_aucpr2_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in utils.polaris_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    else:\n",
    "        raise ValueError(f\"Task {task} not found in any known task list.\")\n",
    "\n",
    "    try:\n",
    "        data = ADME(name = task)\n",
    "    except:\n",
    "        data = Tox(name = task)\n",
    "\n",
    "    split = data.get_split(method = 'scaffold')\n",
    "\n",
    "    train_df = split['train'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    val_df = split['valid'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    test_df = split['test'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "\n",
    "    if metric in ('mae', 'spearman', 'pearson'):\n",
    "        scaler = StandardScaler()\n",
    "        # fit only on train targets\n",
    "        train_vals = train_df[['target']].values\n",
    "        scaler.fit(train_vals)\n",
    "        # add scaled targets\n",
    "        train_df['target'] = scaler.transform(train_vals)\n",
    "        val_df['target']   = scaler.transform(val_df[['target']].values)\n",
    "        test_df['target']  = scaler.transform(test_df[['target']].values)\n",
    "\n",
    "    train_dataloader = make_loader(\n",
    "        df=train_df,\n",
    "        xyz_dir=f'xyz_files/{task}/train/graphs',\n",
    "        target_col='target',\n",
    "        batch_size=32,\n",
    "        num_workers=4,\n",
    "        dtype=torch.float32,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_dataloader = make_loader(\n",
    "        df=val_df,\n",
    "        xyz_dir=f'xyz_files/{task}/val/graphs',\n",
    "        target_col='target',\n",
    "        batch_size=64,\n",
    "        num_workers=1,\n",
    "        dtype=torch.float32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_dataloader = make_loader(\n",
    "        df=test_df,\n",
    "        xyz_dir=f'xyz_files/{task}/test/graphs',\n",
    "        target_col='target',\n",
    "        batch_size=64,\n",
    "        num_workers=1,\n",
    "        dtype=torch.float32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    num_layers = len(model.gnn.encoder.layers)\n",
    "    layer_indices = list(range(0, num_layers))\n",
    "\n",
    "    test_metrics = []\n",
    "    for layer_idx in layer_indices:\n",
    "        print(f'Layer: {layer_idx}')\n",
    "\n",
    "        accumulation_steps = 2\n",
    "        # hyperparameter sweep over learning rates\n",
    "        for lr in [1e-5, 2e-5, 5e-5, 1e-4, 2e-4]:\n",
    "            print(f'LR={lr}')\n",
    "            model = Model(reg_size=32).to('cuda')\n",
    "            optimizer = AdamW(model.parameters(), lr=lr)\n",
    "            epochs = 50\n",
    "            num_training_steps = len(train_dataloader) * epochs\n",
    "            num_warmup_steps = int(0.05 * num_training_steps)\n",
    "            scheduler_warmup = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "            # choose loss fn by task type\n",
    "            if metric in ('mae', 'spearman', 'pearson'):\n",
    "                loss_fn = MSELoss()\n",
    "            else:\n",
    "                loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "            # train for 100 epochs\n",
    "            # For MAE we want to minimize; for others maximize\n",
    "            if metric == 'mae':\n",
    "                best_val = float('inf')\n",
    "            else:\n",
    "                best_val = -float('inf')\n",
    "            best_state = None\n",
    "            best_epoch = -1\n",
    "            for epoch in tqdm(range(epochs)):\n",
    "                model.train()\n",
    "                for step, (mol_graphs, targets) in enumerate(train_dataloader):\n",
    "                    mol_graphs = move_to_device(mol_graphs, device)\n",
    "                    targets = targets.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    _, preds = model(mol_graphs, layer_idx)\n",
    "\n",
    "                    preds_for_loss = preds.squeeze(-1)\n",
    "                    targets_for_loss = targets.squeeze()\n",
    "\n",
    "                    loss = loss_fn(preds_for_loss, targets_for_loss)\n",
    "                    loss = loss / accumulation_steps\n",
    "                    # print(batch)\n",
    "                    # print(preds_for_loss)\n",
    "                    # print(targets_for_loss)\n",
    "                    # print(loss)\n",
    "                    loss.backward()\n",
    "\n",
    "                    # every `accumulation_steps`, do an optimizer step + scheduler step\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        scheduler_warmup.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                # finish off any remaining gradients if dataset size % acc_steps != 0\n",
    "                if (step + 1) % accumulation_steps != 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler_warmup.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # evaluate on validation set\n",
    "                model.eval()\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                val_preds, val_targs = [], []\n",
    "                with torch.no_grad():\n",
    "                    for mol_graphs, targets in val_dataloader:\n",
    "                        mol_graphs = move_to_device(mol_graphs, device)\n",
    "                        targets = targets.to(device)\n",
    "                        _, preds_tensor = model(mol_graphs, layer_idx)\n",
    "\n",
    "                        current_preds_list = preds_tensor.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "                        current_targs_list = targets.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "\n",
    "                        val_preds.extend(current_preds_list)\n",
    "                        val_targs.extend(current_targs_list)\n",
    "\n",
    "                # compute your chosen metric\n",
    "                if metric == 'mae':\n",
    "                    val_score = np.mean(np.abs(np.array(val_preds) - np.array(val_targs)))\n",
    "                elif metric == 'spearman':\n",
    "                    val_score = spearmanr(val_targs, val_preds)[0]\n",
    "                elif metric == 'pearson':\n",
    "                    val_score = pearsonr(val_targs, val_preds)[0]\n",
    "                elif metric == 'auc':\n",
    "                    val_score = roc_auc_score(val_targs, val_preds)\n",
    "                elif metric == 'aucpr':\n",
    "                    val_score = average_precision_score(val_targs, val_preds)\n",
    "\n",
    "                improved = (metric == 'mae' and val_score < best_val) or (metric != 'mae' and val_score > best_val)\n",
    "                if improved:\n",
    "                    best_val   = val_score\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    best_epoch = epoch\n",
    "                    # print(f\"    ↳ new best val_{metric}: {best_val:.4f} (epoch {best_epoch})\")\n",
    "\n",
    "        del(model)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # now evaluate that best model on the test set\n",
    "        model = Model(reg_size=32).to('cuda')\n",
    "        model.load_state_dict(best_state)\n",
    "        model.eval()\n",
    "        test_preds, test_targs = [], []\n",
    "        with torch.no_grad():\n",
    "            for mol_graphs, targets in test_dataloader:\n",
    "                mol_graphs = move_to_device(mol_graphs, device)\n",
    "                targets = targets.to(device)\n",
    "                _, preds_tensor = model(mol_graphs, layer_idx)\n",
    "\n",
    "                current_preds_list = preds_tensor.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "                current_targs_list = targets.view(-1).cpu().numpy().tolist() # Flattens to [B] list\n",
    "\n",
    "                test_preds.extend(current_preds_list)\n",
    "                test_targs.extend(current_targs_list)\n",
    "\n",
    "        if metric in ('mae', 'spearman', 'pearson'):\n",
    "            # bring preds back to original units\n",
    "            test_preds = scaler.inverse_transform(\n",
    "                np.array(test_preds).reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            # use original test targets (unscaled)\n",
    "            test_targs = scaler.inverse_transform(\n",
    "                np.array(test_targs).reshape(-1, 1)\n",
    "            ).flatten()\n",
    "\n",
    "        if metric == 'mae':\n",
    "            test_score = np.mean(np.abs(np.array(test_preds) - np.array(test_targs)))\n",
    "        elif metric == 'spearman':\n",
    "            test_score = spearmanr(test_targs, test_preds)[0]\n",
    "        elif metric == 'pearson':\n",
    "            test_score = pearsonr(test_targs, test_preds)[0]\n",
    "        elif metric == 'auc':\n",
    "            test_score = roc_auc_score(test_targs, test_preds)\n",
    "        elif metric == 'aucpr':\n",
    "            test_score = average_precision_score(test_targs, test_preds)\n",
    "\n",
    "        test_metrics.append(test_score)\n",
    "\n",
    "        print(f\"Layer {layer_idx}: {test_score}\")\n",
    "\n",
    "    # save a DataFrame with one column per task and rows = layer indices\n",
    "    results_df = pd.DataFrame({task: test_metrics}, index=layer_indices)\n",
    "    dfs.append(results_df)\n",
    "    results_df.to_csv(f\"tmp/unimol1_{task}_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
