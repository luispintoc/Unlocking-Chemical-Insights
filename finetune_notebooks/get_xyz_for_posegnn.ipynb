{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af49ca33-7f83-4ab0-bc0f-93ba9134d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade torch_geometric\n",
    "# ! pip install ase==3.24.0\n",
    "# ! pip install torch_nl==0.3\n",
    "# ! pip install rdkit\n",
    "# ! pip install PyTDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7338b-f199-4841-848a-18fe1b508a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "sys.path.append('../external_repos/')\n",
    "from posegnn.model import PosEGNN\n",
    "\n",
    "from ase import Atoms as ASEAtoms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "from tdc.single_pred import ADME, Tox\n",
    "\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e693e-45cf-45b3-ae13-acd19f7b1dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_ase_atoms(smiles: str, random_seed: int = 42,\n",
    "                        jitter_amp: float = 1e-2) -> ASEAtoms | None:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    mol = Chem.AddHs(mol)\n",
    "    \n",
    "    # Try different embedding methods in order of preference\n",
    "    conformer_generated = False\n",
    "    \n",
    "    # Method 1: Standard embedding with UFF optimization\n",
    "    try:\n",
    "        embed_result = AllChem.EmbedMolecule(mol, maxAttempts=2000, randomSeed=random_seed, useRandomCoords=True)\n",
    "        if embed_result == 0:  # Success\n",
    "            # UFF optimization\n",
    "            uff_props = AllChem.UFFGetMoleculeForceField(mol)\n",
    "            if uff_props is not None:\n",
    "                uff_props.Initialize()\n",
    "                uff_props.Minimize(maxIts=1000)\n",
    "                \n",
    "                # Add jitter to avoid symmetry issues\n",
    "                conf = mol.GetConformer()\n",
    "                pos = conf.GetPositions()\n",
    "                np.random.seed(random_seed)\n",
    "                noise = (np.random.rand(*pos.shape) - 0.5) * jitter_amp\n",
    "                new_pos = pos + noise\n",
    "                for i, p in enumerate(new_pos):\n",
    "                    conf.SetAtomPosition(i, p.tolist())\n",
    "                \n",
    "                # Final MMFF optimization\n",
    "                AllChem.MMFFOptimizeMolecule(mol)\n",
    "                conformer_generated = True\n",
    "    except Exception as e:\n",
    "        print(f\"Method 1 failed: {e}\")\n",
    "        pass\n",
    "    \n",
    "    # Method 2: Try ETKDGv3 if method 1 failed\n",
    "    if not conformer_generated:\n",
    "        try:\n",
    "            embed_result = AllChem.EmbedMolecule(mol, AllChem.ETKDGv3())\n",
    "            if embed_result == 0:\n",
    "                conformer_generated = True\n",
    "        except Exception as e:\n",
    "            print(f\"Method 2 failed: {e}\")\n",
    "            pass\n",
    "    \n",
    "    # Method 3: Try explicit conformer creation with 2D coords\n",
    "    if not conformer_generated:\n",
    "        try:\n",
    "            # Create a fresh conformer explicitly\n",
    "            conf = Chem.Conformer(mol.GetNumAtoms())\n",
    "            for i in range(mol.GetNumAtoms()):\n",
    "                conf.SetAtomPosition(i, [0.0, 0.0, 0.0])  # Initialize with zeros\n",
    "            conf_id = mol.AddConformer(conf)\n",
    "            \n",
    "            # Now compute 2D coords\n",
    "            AllChem.Compute2DCoords(mol)\n",
    "            \n",
    "            # Check if the conformer exists and has non-zero coordinates\n",
    "            if mol.GetNumConformers() > 0:\n",
    "                conformer_generated = True\n",
    "        except Exception as e:\n",
    "            print(f\"Method 3 failed: {e}\")\n",
    "            pass\n",
    "\n",
    "    # Bail out if we couldn't generate any conformer\n",
    "    if not conformer_generated or mol.GetNumConformers() == 0:\n",
    "        return None\n",
    "\n",
    "    # Get the conformer\n",
    "    conf = mol.GetConformer()\n",
    "\n",
    "    def has_overlaps(pos, eps=1e-6):\n",
    "        d = torch.cdist(torch.tensor(pos), torch.tensor(pos))\n",
    "        n = d.shape[0]\n",
    "        d[range(n), range(n)] = float('inf')\n",
    "        return (d <= eps).any().item()\n",
    "\n",
    "    pos = conf.GetPositions()\n",
    "    if has_overlaps(pos, eps=1e-6):\n",
    "        # you could loop a few times here with new seeds,\n",
    "        # or simply bail out and let your dataset drop it:\n",
    "        return None\n",
    "    \n",
    "    # Extract heavy atoms only\n",
    "    heavy_atoms = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        if atom.GetSymbol() != 'H':\n",
    "            idx = atom.GetIdx()\n",
    "            try:\n",
    "                pos = conf.GetAtomPosition(idx)\n",
    "                heavy_atoms.append((atom.GetSymbol(), pos))\n",
    "            except Exception:\n",
    "                # Skip this atom if position can't be retrieved\n",
    "                continue\n",
    "\n",
    "    # If no heavy atoms were successfully processed, fail\n",
    "    if not heavy_atoms:\n",
    "        return None\n",
    "\n",
    "    # Final check\n",
    "    mol = Chem.RemoveHs(mol)\n",
    "    conf = mol.GetConformer()\n",
    "    pos = conf.GetPositions()\n",
    "    # if np.allclose(pos, 0) or np.allclose(pos[:, 2], 0) or np.isnan(pos).any():\n",
    "    #     return None\n",
    "\n",
    "    return ASEAtoms(\n",
    "        symbols=[a.GetSymbol() for a in mol.GetAtoms()],\n",
    "        positions=pos,\n",
    "        pbc=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca27adcc-a1cf-4041-a2a9-465e1528d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_from_ase(atoms: ASEAtoms) -> Data:\n",
    "    z = torch.tensor(atoms.get_atomic_numbers(), dtype=torch.long)\n",
    "    box = torch.tensor(atoms.get_cell().tolist()).unsqueeze(0).float()\n",
    "    pos = torch.tensor(atoms.get_positions().tolist()).float()\n",
    "    batch = torch.zeros(len(z), dtype=torch.long)\n",
    "    return Data(z=z, pos=pos, box=box, batch=batch, num_graphs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb779624-688a-45d5-9631-cf91cfef0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                reg_drop_rate=0.1,\n",
    "                intermediate_size=256,\n",
    "                num_targets=1):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        self.reg_drop_rate = reg_drop_rate\n",
    "        self.num_targets = num_targets\n",
    "        self.intermediate_size = intermediate_size\n",
    "\n",
    "        self.hidden_size = 256\n",
    "        checkpoint_dict = torch.load('../external_repos/posegnn/pytorch_model.bin', weights_only=True, map_location=device)\n",
    "        self.transformer = PosEGNN(checkpoint_dict[\"config\"])\n",
    "        self.transformer.load_state_dict(checkpoint_dict[\"state_dict\"], strict=False)\n",
    "\n",
    "    def forward(self, batch, layer_idx=-1):\n",
    "        with torch.no_grad():\n",
    "            output_dict = self.transformer(batch)\n",
    "            node_emb = output_dict[\"embedding_0\"]\n",
    "            if node_emb.dim() == 3:\n",
    "                node_emb = node_emb[:, :, layer_idx]  # -> [total_nodes, hidden_dim]\n",
    "            # `batch.batch` is the graph‐index for each node\n",
    "            graph_emb = global_mean_pool(node_emb, batch.batch) # -> [batch_size, hidden_dim]\n",
    "        return graph_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce795364",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d4128-6ae4-4721-8782-0e95550fb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "base_dir = '../input_data/tdcommons/admet_group'\n",
    "dfs = []\n",
    "\n",
    "for task in os.listdir(base_dir):\n",
    "    if task.startswith('.'): # remove .ipy file\n",
    "        continue\n",
    "\n",
    "    print(task)\n",
    "\n",
    "    if task in skip:\n",
    "        continue\n",
    "\n",
    "    prefix = 'tdcommons/'\n",
    "    if prefix+task in utils.tdc_mae_tasks:\n",
    "        metric = 'mae'\n",
    "    elif prefix+task in utils.tdc_spearman_task:\n",
    "        metric = 'spearman'\n",
    "    elif prefix+task in utils.polaris_pearson_tasks:\n",
    "        metric = 'pearson'\n",
    "    elif prefix+task in utils.tdc_auroc_tasks:\n",
    "        metric = 'auc'\n",
    "    elif prefix+task in utils.tdc_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in utils.tdc_aucpr2_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in utils.polaris_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    else:\n",
    "        raise ValueError(f\"Task {task} not found in any known task list.\")\n",
    "\n",
    "    try:\n",
    "        data = ADME(name = task)\n",
    "    except:\n",
    "        data = Tox(name = task)\n",
    "\n",
    "    split = data.get_split(method = 'scaffold')\n",
    "\n",
    "    train_df = split['train'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    val_df = split['valid'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    test_df = split['test'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "\n",
    "    if metric in ('mae', 'spearman', 'pearson'):\n",
    "        scaler = StandardScaler()\n",
    "        # fit only on train targets\n",
    "        train_vals = train_df[['target']].values\n",
    "        scaler.fit(train_vals)\n",
    "        # add scaled targets\n",
    "        train_df['target'] = scaler.transform(train_vals)\n",
    "        val_df['target']   = scaler.transform(val_df[['target']].values)\n",
    "        test_df['target']  = scaler.transform(test_df[['target']].values)\n",
    "\n",
    "    for df, split in zip([train_df, val_df, test_df], ['train','val','test']):\n",
    "        out_dir = f'3D_data/{task}/{split}/graphs'\n",
    "\n",
    "        # If the folder already exists *and* has at least one .pt file, skip it\n",
    "        if os.path.isdir(out_dir) and any(f.endswith('.pt') for f in os.listdir(out_dir)):\n",
    "            continue\n",
    "\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            atoms = smiles_to_ase_atoms(row.smiles)\n",
    "            if atoms is None:\n",
    "                continue\n",
    "            data = build_data_from_ase(atoms) \n",
    "            assert data.num_nodes > 0, f\"no atoms in idx={idx}\"\n",
    "            data.y = torch.tensor([row.target], dtype=torch.float32)\n",
    "\n",
    "            if torch.isnan(data.pos).any() or torch.isinf(data.pos).any():\n",
    "                print(f\"WARNING: NaN or Inf in positions for SMILES: {row.smiles}, idx: {idx}\")\n",
    "                # Decide how to handle: skip, log, etc.\n",
    "                continue\n",
    "            if torch.isnan(data.y).any() or torch.isinf(data.y).any():\n",
    "                print(f\"WARNING: NaN or Inf in target for SMILES: {row.smiles}, idx: {idx}\")\n",
    "                # This might indicate issues with the scaler or original data\n",
    "                continue\n",
    "\n",
    "            if not torch.isfinite(data.y).all():\n",
    "                print(f\"Skipping idx={idx} due to NaN target\")\n",
    "                continue\n",
    "\n",
    "            torch.save(data, os.path.join(out_dir, f'{idx}.pt'))\n",
    "        print(f\"✔ Finished dumping {len(os.listdir(out_dir))} graphs for {task}/{split}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
