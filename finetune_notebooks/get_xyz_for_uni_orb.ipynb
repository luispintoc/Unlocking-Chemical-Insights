{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04284cbe-f32e-498a-9802-2f606452b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install orb-models\n",
    "# ! pip install \"pynanoflann@git+https://github.com/dwastberg/pynanoflann#egg=af434039ae14bedcbb838a7808924d6689274168\",\n",
    "# ! pip install rdkit==2023.9.4\n",
    "# ! pip install xyz2graph\n",
    "# ! pip install nglview\n",
    "# ! pip install packaging==24.1\n",
    "# ! pip install transformers==4.46.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b94092-ab83-4b85-8368-b16f73b26cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade torch_geometric\n",
    "# ! pip install ase==3.24.0\n",
    "# ! pip install torch_nl==0.3\n",
    "# ! pip install rdkit\n",
    "# ! pip install PyTDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0805dab-b886-476a-8741-b662f94da1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "import os, sys, gc\n",
    "import pandas as pd\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import ase\n",
    "from ase import Atoms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import orb_models.utils as utils\n",
    "from orb_models.forcefield.base import AtomGraphs\n",
    "from orb_models.dataset.augmentations import rotate_randomly\n",
    "from orb_models.dataset.base_datasets import AtomsDataset\n",
    "sys.path.append('../external_repos/')\n",
    "from orb_models_modified.orb_models.forcefield import base, pretrained, atomic_system, property_definitions\n",
    "\n",
    "sys.path.append('../')\n",
    "import utils as tdc_utils\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "from tdc.single_pred import ADME, Tox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a6354-7d6c-4f88-bab3-a3cbe3068cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype  = torch.float32\n",
    "compile = None\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32  = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde844c-f17c-425e-ac3e-81fab86e04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def smiles_to_xyz_file(smiles: str, output_dir: str = 'xyz_files/', random_seed: int = 42,\n",
    "                      jitter_amp: float = 1e-2) -> str | None:\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert SMILES to 3D structure with hydrogens (for better geometry)\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "\n",
    "    mol = Chem.AddHs(mol)\n",
    "\n",
    "    # Try different embedding methods in order of preference\n",
    "    conformer_generated = False\n",
    "    \n",
    "    # Method 1: Standard embedding with UFF optimization\n",
    "    try:\n",
    "        embed_result = AllChem.EmbedMolecule(mol, maxAttempts=2000, randomSeed=random_seed, useRandomCoords=True)\n",
    "        if embed_result == 0:  # Success\n",
    "            # UFF optimization\n",
    "            uff_props = AllChem.UFFGetMoleculeForceField(mol)\n",
    "            if uff_props is not None:\n",
    "                uff_props.Initialize()\n",
    "                uff_props.Minimize(maxIts=1000)\n",
    "\n",
    "                # Add jitter to avoid symmetry issues\n",
    "                conf = mol.GetConformer()\n",
    "                pos = conf.GetPositions()\n",
    "                np.random.seed(random_seed)\n",
    "                noise = (np.random.rand(*pos.shape) - 0.5) * jitter_amp\n",
    "                new_pos = pos + noise\n",
    "                for i, p in enumerate(new_pos):\n",
    "                    conf.SetAtomPosition(i, p.tolist())\n",
    "\n",
    "                # Final MMFF optimization\n",
    "                AllChem.MMFFOptimizeMolecule(mol)\n",
    "                conformer_generated = True\n",
    "    except Exception as e:\n",
    "        print(f\"Method 1 failed: {e}\")\n",
    "        pass\n",
    "    \n",
    "    # Method 2: Try ETKDGv3 if method 1 failed\n",
    "    if not conformer_generated:\n",
    "        try:\n",
    "            embed_result = AllChem.EmbedMolecule(mol, AllChem.ETKDGv3())\n",
    "            if embed_result == 0:\n",
    "                conformer_generated = True\n",
    "        except Exception as e:\n",
    "            print(f\"Method 2 failed: {e}\")\n",
    "            pass\n",
    "    \n",
    "    # Method 3: Try explicit conformer creation with 2D coords\n",
    "    if not conformer_generated:\n",
    "        try:\n",
    "            # Create a fresh conformer explicitly\n",
    "            conf = Chem.Conformer(mol.GetNumAtoms())\n",
    "            for i in range(mol.GetNumAtoms()):\n",
    "                conf.SetAtomPosition(i, [0.0, 0.0, 0.0])  # Initialize with zeros\n",
    "            conf_id = mol.AddConformer(conf)\n",
    "            \n",
    "            # Now compute 2D coords\n",
    "            AllChem.Compute2DCoords(mol)\n",
    "            \n",
    "            # Check if the conformer exists and has non-zero coordinates\n",
    "            if mol.GetNumConformers() > 0:\n",
    "                conformer_generated = True\n",
    "        except Exception as e:\n",
    "            print(f\"Method 3 failed: {e}\")\n",
    "            pass\n",
    "\n",
    "    # Bail out if we couldn't generate any conformer\n",
    "    if not conformer_generated or mol.GetNumConformers() == 0:\n",
    "        return None\n",
    "\n",
    "    # Get the conformer\n",
    "    conf = mol.GetConformer()\n",
    "\n",
    "    # 5) Check for overlaps & retry if needed\n",
    "    def has_overlaps(pos, eps=1e-6):\n",
    "        d = torch.cdist(torch.tensor(pos), torch.tensor(pos))\n",
    "        n = d.shape[0]\n",
    "        d[range(n), range(n)] = float('inf')\n",
    "        return (d <= eps).any().item()\n",
    "\n",
    "    pos = conf.GetPositions()\n",
    "    if has_overlaps(pos, eps=1e-6):\n",
    "        # you could loop a few times here with new seeds,\n",
    "        # or simply bail out and let your dataset drop it:\n",
    "        return None\n",
    "    \n",
    "    # Extract heavy atoms only\n",
    "    heavy_atoms = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        if atom.GetSymbol() != 'H':\n",
    "            idx = atom.GetIdx()\n",
    "            try:\n",
    "                pos = conf.GetAtomPosition(idx)\n",
    "                heavy_atoms.append((atom.GetSymbol(), pos))\n",
    "            except Exception:\n",
    "                # Skip this atom if position can't be retrieved\n",
    "                continue\n",
    "\n",
    "    # If no heavy atoms were successfully processed, fail\n",
    "    if not heavy_atoms:\n",
    "        return None\n",
    "\n",
    "    # Build filename\n",
    "    h = hashlib.md5(smiles.encode()).hexdigest()[:10]\n",
    "    filename = f\"{h}.xyz\"\n",
    "    path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Write out XYZ\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(f\"{len(heavy_atoms)}\\n\")\n",
    "        f.write(f\"{smiles}\\n\")\n",
    "        for sym, pt in heavy_atoms:\n",
    "            f.write(f\"{sym} {pt.x:.6f} {pt.y:.6f} {pt.z:.6f}\\n\")\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d81605-43ff-41b7-b7ef-f5e4e52f5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_smiles(smiles):\n",
    "    \"\"\"Converts a SMILES string to its canonical form.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None  # or raise an exception, depending on your needs\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8072b3-a0b7-4588-b820-524c5222e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smiles(input_file):\n",
    "    # Read the entire content of the XYZ file.\n",
    "    with open(input_file, 'r') as f:\n",
    "        xyz_data = f.read()\n",
    "\n",
    "    # Split the content into lines.\n",
    "    lines = xyz_data.splitlines()\n",
    "    smiles = lines[1].strip()\n",
    "\n",
    "    return smiles #canonicalize_smiles(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6526a789-0ad6-4dc2-a82f-7a4c7afbbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XYZFolderDataset(AtomsDataset):\n",
    "    \"\"\"\n",
    "    A Dataset that mimics AseSqliteDataset but reads single .xyz files\n",
    "    based on metal_smiles from a DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,                        # your pandas DataFrame\n",
    "        xyz_dir: Union[str, Path],\n",
    "        system_config: atomic_system.SystemConfig,\n",
    "        target_col: str,\n",
    "        target_config: Optional[property_definitions.PropertyConfig] = None,\n",
    "        augmentations: Optional[List[Callable[[ase.Atoms], None]]] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ):\n",
    "        super().__init__(name=\"xyz_folder\", system_config=system_config, augmentations=augmentations)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.xyz_dir = Path(xyz_dir)\n",
    "        self.target_col = target_col\n",
    "        self.dtype = dtype or torch.get_default_dtype()\n",
    "        self.target_config = target_config or property_definitions.PropertyConfig()\n",
    "        self.constraints: List[Callable] = []\n",
    "\n",
    "        # build smiles → path lookup\n",
    "        self.xyz_dir = Path(xyz_dir)\n",
    "        lookup = {}\n",
    "        for p in self.xyz_dir.glob(\"*.xyz\"):\n",
    "            s = get_smiles(str(p))\n",
    "            lookup[s] = p\n",
    "        self._lookup = lookup\n",
    "\n",
    "        df = df.reset_index(drop=True)\n",
    "        mask = df['smiles'].isin(lookup)\n",
    "        num_bad = (~mask).sum()\n",
    "        if num_bad > 0:\n",
    "            print(f\"Warning: dropping {num_bad} rows with no matching .xyz file\")\n",
    "        self.df = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[AtomGraphs, torch.Tensor]:\n",
    "        row = self.df.iloc[idx]\n",
    "        smiles = row['smiles']\n",
    "        target = row[self.target_col]\n",
    "\n",
    "        xyz_path = self._lookup.get(smiles)\n",
    "        if xyz_path is None:\n",
    "            raise KeyError(f\"Could not find .xyz for SMILES `{smiles}` in {self.xyz_dir}\")\n",
    "\n",
    "        # read Atoms\n",
    "        atoms = ase.io.read(str(xyz_path))\n",
    "        # extract targets into atoms.info\n",
    "        atoms.info = {}\n",
    "        atoms.info.update(self.target_config.extract(row.to_dict(), self.name, \"targets\"))\n",
    "\n",
    "        # augmentations\n",
    "        for aug in self.augmentations or []:\n",
    "            aug(atoms)\n",
    "\n",
    "        # constraints\n",
    "        atoms.set_constraint(None)\n",
    "        for c in self.constraints:\n",
    "            c(atoms, {}, self.name)\n",
    "\n",
    "        # to graph\n",
    "        graph = atomic_system.ase_atoms_to_atom_graphs(\n",
    "            atoms=atoms,\n",
    "            system_config=self.system_config,\n",
    "            edge_method=\"knn_scipy\",\n",
    "            wrap=True,\n",
    "            system_id=idx,\n",
    "            output_dtype=self.dtype,\n",
    "            graph_construction_dtype=self.dtype,\n",
    "        )\n",
    "\n",
    "        # return graph + scalar target\n",
    "        return graph, torch.tensor(target, dtype=self.dtype)\n",
    "\n",
    "    def get_atom(self, idx: int) -> ase.Atoms:\n",
    "        \"\"\"Return the raw ASE Atoms for this index.\"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "        xyz_path = self._lookup[row['smiles']]\n",
    "        return ase.io.read(str(xyz_path))\n",
    "\n",
    "    def get_atom_and_metadata(self, idx: int) -> Tuple[ase.Atoms, Dict]:\n",
    "        \"\"\"Return both ASE Atoms and the row’s metadata dict.\"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "        xyz_path = self._lookup[row['smiles']]\n",
    "        atoms = ase.io.read(str(xyz_path))\n",
    "        return atoms, row.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ebe037-ff92-43e0-8daa-d05798fbff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_graph_and_targets(batch):\n",
    "    # batch is a list of (AtomGraphs, tensor) tuples\n",
    "    graphs, targets = zip(*batch)\n",
    "    batched_graph = base.batch_graphs(graphs)\n",
    "    batched_targets = torch.stack(targets, dim=0)\n",
    "    return batched_graph, batched_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe741e6-f968-405f-a6ba-2dab7c265088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_loader(\n",
    "    df,\n",
    "    xyz_dir: str,\n",
    "    system_config,\n",
    "    target_col: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    target_config=None,\n",
    "    augmentations=None,\n",
    "    dtype=None,\n",
    "):\n",
    "    dataset = XYZFolderDataset(\n",
    "        df=df,\n",
    "        xyz_dir=xyz_dir,\n",
    "        system_config=system_config,\n",
    "        target_col=target_col,\n",
    "        target_config=target_config,\n",
    "        augmentations=augmentations,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    # sampler = RandomSampler(dataset)\n",
    "    # batch_sampler = BatchSampler(sampler, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=utils.worker_init_fn,\n",
    "        collate_fn=collate_graph_and_targets,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "def make_val_loader(\n",
    "    df,\n",
    "    xyz_dir: str,\n",
    "    system_config,\n",
    "    target_col: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    target_config=None,\n",
    "    augmentations=None,\n",
    "    dtype=None,\n",
    "):\n",
    "    dataset = XYZFolderDataset(\n",
    "        df=df,\n",
    "        xyz_dir=xyz_dir,\n",
    "        system_config=system_config,\n",
    "        target_col=target_col,\n",
    "        target_config=target_config,\n",
    "        augmentations=augmentations,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        worker_init_fn=utils.worker_init_fn,\n",
    "        collate_fn=collate_graph_and_targets,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f7f28-e629-4a8a-b5b8-663278d47393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                reg_drop_rate=0.1,\n",
    "                reg_size=256,\n",
    "                num_labels=1):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        self.reg_drop_rate = reg_drop_rate\n",
    "        self.num_targets = num_labels\n",
    "        self.reg_size = reg_size\n",
    "\n",
    "        # self.gnn = pretrained.orb_v3_direct_inf_mpa(\n",
    "        # device='cpu',\n",
    "        # precision=\"float32-highest\",   # or \"float32-highest\" / \"float64\n",
    "        # )\n",
    "        self.gnn = pretrained.orb_v3_conservative_inf_omat(\n",
    "        device='cpu',\n",
    "        precision=\"float32-highest\",   # or \"float32-highest\" / \"float64\n",
    "        )\n",
    "\n",
    "    def forward(self, batched_graph, layer_idx):\n",
    "        node_emb = self.gnn(batched_graph)\n",
    "        node_emb = node_emb[\"intermediate_layers\"][layer_idx]\n",
    "        batch_index = batched_graph._get_per_node_graph_indices().long()\n",
    "        graph_emb = global_mean_pool(node_emb, batch_index)\n",
    "        return graph_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d17493-0fe2-4a03-97e5-8f801ea27d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to('cuda')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e3a59-b153-4699-a97e-94c14d4a63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip = ['solubility_aqsoldb', 'ppbr_az',\n",
    "#        'herg', 'cyp2d6_substrate_carbonmangels',\n",
    "#        'bbb_martins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eace93e-5876-4df1-89a9-60e9a07170a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "base_dir = 'tdcommons/admet_group/'\n",
    "dfs = []\n",
    "\n",
    "for task in os.listdir(base_dir):\n",
    "    if task.startswith('.'):\n",
    "        continue\n",
    "\n",
    "    print(task)\n",
    "\n",
    "    task_dir = os.path.join(base_dir, task)\n",
    "\n",
    "    # if task in skip: continue\n",
    "\n",
    "    prefix = 'tdcommons/'\n",
    "    if prefix+task in tdc_utils.tdc_mae_tasks:\n",
    "        metric = 'mae'\n",
    "    elif prefix+task in tdc_utils.tdc_spearman_task:\n",
    "        metric = 'spearman'\n",
    "    elif prefix+task in tdc_utils.polaris_pearson_tasks:\n",
    "        metric = 'pearson'\n",
    "    elif prefix+task in tdc_utils.tdc_auroc_tasks:\n",
    "        metric = 'auc'\n",
    "    elif prefix+task in tdc_utils.tdc_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in tdc_utils.tdc_aucpr2_tasks:\n",
    "        metric = 'aucpr'\n",
    "    elif prefix+task in tdc_utils.polaris_aucpr_tasks:\n",
    "        metric = 'aucpr'\n",
    "    else:\n",
    "        raise ValueError(f\"Task {task} not found in any known task list.\")\n",
    "\n",
    "    try:\n",
    "        data = ADME(name = task)\n",
    "    except:\n",
    "        data = Tox(name = task)\n",
    "\n",
    "    split = data.get_split(method = 'scaffold')\n",
    "\n",
    "    train_df = split['train'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    val_df = split['valid'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "    test_df = split['test'].rename({'Drug': 'smiles', 'Y': 'target'}, axis=1).drop('Drug_ID', axis=1)\n",
    "\n",
    "    train_dataloader = make_train_loader(\n",
    "        df=train_df,\n",
    "        xyz_dir=f'xyz_files/{task}/train/graphs',\n",
    "        system_config=model.gnn.system_config,\n",
    "        target_col='target',\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        target_config=None,\n",
    "        augmentations=[rotate_randomly],\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    test_dataloader = make_val_loader(\n",
    "        df=test_df,\n",
    "        xyz_dir=f'xyz_files/{task}/test/graphs',\n",
    "        system_config=model.gnn.system_config,\n",
    "        target_col='target',\n",
    "        batch_size=256,\n",
    "        num_workers=1,\n",
    "        target_config=None,\n",
    "        augmentations=None,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    num_layers = 5\n",
    "    layer_indices = list(range(0, num_layers))\n",
    "\n",
    "    for df, split, loader in zip([train_df, test_df], ['train', 'test'], [train_dataloader, test_dataloader]):\n",
    "        for layer_idx in layer_indices:\n",
    "            all_embeddings = []\n",
    "            for step, (mol_graphs, targets) in enumerate(loader):\n",
    "                mol_graphs = mol_graphs.to(device)\n",
    "                embeddings = model(mol_graphs, layer_idx)\n",
    "                embeddings = embeddings.detach().cpu().numpy()\n",
    "                all_embeddings.append(embeddings)\n",
    "\n",
    "            stacked = np.vstack(all_embeddings)\n",
    "\n",
    "            n_samples, emb_dim = stacked.shape\n",
    "            col_names = [f\"orb_conserv_layer{layer_idx}_{i}\" for i in range(emb_dim)]\n",
    "            out_df = pd.DataFrame(stacked, columns=col_names)\n",
    "            combined = pd.concat([df, out_df], axis=1)\n",
    "            combined.columns = list(df.columns) + list(out_df.columns)\n",
    "\n",
    "            n = f\"{split}_orbconserv_layer{layer_idx}_emb.csv\"\n",
    "            combined.to_csv(os.path.join(task_dir, n), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
